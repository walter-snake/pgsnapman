#!/bin/bash

# pgsnap_dump: starts a single postgres dump operation
#                     must run on a backup node, with passwordless superuser access (.pgpass does the job)
#
# $1 pgsql instance id <pginstanceid_pgdns_pgport>, resolves to snapshot/pgsqlinstance directory (must contain the job list)
# $2 dump job id
# $3 verbosity [VERBOSE|SILENT]
# $4 startup type [CRON|SINGLE]
#      CRON will use cached job information in <snapshots>/<serverinstance>/dumpjobs.list
#      SINGLE use temp cached information in <temp>/$5.0.singledumpjobs.list
# $5 single run process id (for linking temp job info cache)
#
# Actual backups are usually started by cron on this machine, but you may invoke it manually. This will override
# HALTED jobs in CRON mode.

# ======================================
# Initialization
# ======================================
PGSQLID=$1
JOBID=$2
VERBOSITY=$3
STARTTYPE=$4
SINGLERUNPID=$5

# Get the script directory (must do this first)
SOURCE="${BASH_SOURCE[0]}"
while [ -h "$SOURCE" ]; do # resolve $SOURCE until the file is no longer a symlink
  DIR="$( cd -P "$( dirname "$SOURCE" )" && pwd )"
  SOURCE="$(readlink "$SOURCE")"
  [[ $SOURCE != /* ]] && SOURCE="$DIR/$SOURCE" # if $SOURCE was a relative symlink, we need to resolve it relative to the path where the symlink file was located
done
SCRIPTPATH="$( cd -P "$( dirname "$SOURCE" )" && pwd )"

# Check cmd args
if [ "${STARTTYPE}" == "" ]; then
  echo "ERROR pgsnap_dump no startup type provided"
  cleanupexit 1
fi
if [ "${STARTTYPE}" == "SINGLE" ] && [ "${SINGLERUNPID}" == "" ]; then
  echo "ERROR pgsnap_dump needs a process id in SINGLE mode"
  exit 1
fi

# Catalog database needed? YES when running in SINGLE MODE, we must check if a job may start (job status ACTIVE)
if [ "${STARTTYPE}" == "CRON" ]; then
  PGSCDB_REQUIRED=NO
else
  PGSCDB_REQUIRED=YES
fi

# Load functions, will also perform initialization operations
. ${SCRIPTPATH}/pgsnap_lib

# ============================
# Functions
# ============================

# Prepare for starting a custom script
# Writes out all current variables to a temp file
# Downloads required script
# $1 script name
function preparescriptrun {
  echo "JOBID=${jobid}" > ${TEMPDIR}/$$.${JOBID}.pgsnap_dump_scriptinit.temp
  echo "INITIMESTAMP=${INITIMESTAMP}" >> ${TEMPDIR}/$$.${JOBID}.pgsnap_dump_scriptinit.temp
  echo "VERBOSITY=${VERBOSITY}" >> ${TEMPDIR}/$$.${JOBID}.pgsnap_dump_scriptinit.temp
  echo "STARTTYPE=${STARTTYPE}" >> ${TEMPDIR}/$$.${JOBID}.pgsnap_dump_scriptinit.temp
  echo "SINGLERUNPID=${SINGLERUNPID}" >> ${TEMPDIR}/$$.${JOBID}.pgsnap_dump_scriptinit.temp
  echo "JOBDIR=${jobdir}" >> ${TEMPDIR}/$$.${JOBID}.pgsnap_dump_scriptinit.temp
  echo "BUNAME=${buname}" >> ${TEMPDIR}/$$.${JOBID}.pgsnap_dump_scriptinit.temp
  echo "PGSQLHOST=\"${pgsqlhost}\"" >> ${TEMPDIR}/$$.${JOBID}.pgsnap_dump_scriptinit.temp
  echo "PGSQLPORT=${pgsqlport}" >> ${TEMPDIR}/$$.${JOBID}.pgsnap_dump_scriptinit.temp
  echo "PGSQLSUPERUSER=${pgsqlsuperuser}" >> ${TEMPDIR}/$$.${JOBID}.pgsnap_dump_scriptinit.temp
  echo "PGSQLDBNAME=${dbname}" >> ${TEMPDIR}/$$.${JOBID}.pgsnap_dump_scriptinit.temp
  echo "SCHEMA=${schema}" >> ${TEMPDIR}/$$.${JOBID}.pgsnap_dump_scriptinit.temp
  echo "ROOTDIR=${ROOTDIR}" >> ${TEMPDIR}/$$.${JOBID}.pgsnap_dump_scriptinit.temp
  echo "LOGDIR=${LOGDIR}" >> ${TEMPDIR}/$$.${JOBID}.pgsnap_dump_scriptinit.temp
  echo "TEMPDIR=${TEMPDIR}" >> ${TEMPDIR}/$$.${JOBID}.pgsnap_dump_scriptinit.temp
  echo "UPLOADDIR=${UPLOADDIR}" >> ${TEMPDIR}/$$.${JOBID}.pgsnap_dump_scriptinit.temp
  echo "PGBIN=${pgbin}" >> ${TEMPDIR}/$$.${JOBID}.pgsnap_dump_scriptinit.temp
  echo "PGSNAPMANLOG=${PGSNAPMANLOG}" >> ${TEMPDIR}/$$.${JOBID}.pgsnap_dump_scriptinit.temp
  echo "TOOL_LOG=${TOOL_LOG}" >> ${TEMPDIR}/$$.${JOBID}.pgsnap_dump_scriptinit.temp
  if [ "${jobdir}" == "" ] || [ "${buname}" == "" ]; then
    ERRCODE=1
  else   
    mkdir ${jobdir}/${buname}
    # Download script
    ${PGSCBIN}/psql ${PGSCHOST} -p ${PGSCPORT} -U ${PGSCUSER} --dbname=${PGSCDB} -A -t -c "SELECT get_scriptcode ('${1}');" > ${SCRIPTDIR}/${1}
  fi
}

# Get the global ACL's from the database (stored in sql format)
# $1 database name
# $2 store in file (full path)
function dumpglobalacls {
  local line
  # Check name
  if [ "${1}" == *";"* ]; then
    return
  fi

  # The nice way, using a function (but that's not present in every database)
  # local sql="select * from get_globalacl('${1}') as (datname name, acl_role text, acl_rights text);"
  # Ugly but acceptable: we checked for ; in the name, but probably nothing will work with a weird database name (you'll have to connect in the first place)
  local sql="with acls as (select datname, split_part(acl, '=', 1) as acl_role \
  , (regexp_split_to_array(acl, '[=/]'))[2] as acl_rights \
from (select datname, unnest(datacl)::text as acl \
  from pg_database) a \
) \
select datname,acl_role,acl_rights from acls \
where datname = '${1}'"
  local acls=`${pgbin}/psql ${pgsqlhost} -p ${pgsqlport} -U ${pgsqlsuperuser} --dbname="${1}" -F '|' -A -t -c "${sql}"`
  touch ${2}
  if [ "${acls}" != "" ]; then
    echo "REVOKE ALL ON DATABASE ${1} FROM public;" >> ${2}
  fi
  for line in $acls; do
    local acl="$(getfieldvalue ${line} 3)"
    local role="$(getfieldvalue ${line} 2)"
    if [ "${role}" == "" ]; then
      role="public"
    fi
    echo "REVOKE ALL ON DATABASE ${1} FROM ${role};" >> ${2}
    if [[ "${acl}" =~ "C" ]]; then
      echo "GRANT CREATE ON DATABASE ${1} TO ${role};" >> ${2}
    fi
    if [[ "${acl}" =~ "T" ]]; then
      echo "GRANT TEMP ON DATABASE ${1} TO ${role};" >> ${2}
    fi
    if [[ "${acl}" =~ "c" ]]; then
      echo "GRANT CONNECT ON DATABASE ${1} TO ${role};" >> ${2}
    fi
  done
}

# Dump the database settings (raw table format, | separated)
# $1 database name
# $2 export file
function dumpdbsettings {
  # Check name
  if [ "${1}" == *";"* ]; then
    return
  fi
 
  local acls=`${pgbin}/psql ${pgsqlhost} -p ${pgsqlport} -U ${pgsqlsuperuser} --dbname="${1}" -F '|' -A -t -c "${sql}"`
  # The good query
  sql="SELECT d.datname, pg_catalog.pg_get_userbyid(d.datdba) as owner, pg_catalog.pg_encoding_to_char(d.encoding) as encoding, \
    d.datcollate as collate, d.datctype as ctype, pg_catalog.array_to_string(d.datacl, ',') AS acl_rights \
    , spcname as dattablespace \
    FROM pg_catalog.pg_database d \
    LEFT JOIN (SELECT oid, spcname FROM pg_catalog.pg_tablespace WHERE spcname NOT LIKE 'pg_default') t \
      ON d.dattablespace = t.oid \
    WHERE d.datname = '$1';"

  ${pgbin}/psql ${pgsqlhost} -p ${pgsqlport} -U ${pgsqlsuperuser} --dbname="${1}" -F '|' -A -t -c "${sql}" > ${2}
}

# Start a specific dump job 
# $1 path to postgres instance snapshot root directory
# $2 id of job to start
function startpgsnapjob {
  # Determine job type, as naming schema's differ, also set --schema-only flag if required
  if [ "${butype}" == "CLUSTER_SCHEMA" ]; then
    jobdir=${DUMPSNAP}/${1}/${jobid}_cluster.schema
  elif [ "${butype}" == "CLUSTER" ]; then
    jobdir=${DUMPSNAP}/${1}/${jobid}_cluster.full
  else
    jobdir=${DUMPSNAP}/${1}/${jobid}_${dbname}${schema_part}
  fi
  if [[ "${butype}" =~ .*SCHEMA ]]; then
    schemaonly="--schema-only"
  fi 
  mkdir -p ${jobdir}

  # Set logfile
  TOOL_LOG=${jobdir}.log

  # Get filename, postgres version
  buname=${jobid}_${dbname}${schema_part}_${butype}_`date '+%Y%m%dT%H%M%S'`
  log "INFO" "job id ${jobid} - ${jobtype} backup ${buname}"
  # Check postgres server version and binary availability 
  pgversion=$(getpgversion "${pgsqlhost}" "${pgsqlport}" "${pgsqlsuperuser}" "${dbname}")
  if [ "${pgversion}" == "" ]; then
    MSG="job id ${jobid} - could not connect to postgres instance: ${pgsqlsuperuser}@${pgsqlfqdn}:${pgsqlport}/${dbname}"
    log "ERROR" "${MSG}"
    ERRORS=1
  else
    eval pgbinvar=PGBIN${pgversion}
    pgbin=${!pgbinvar}
  fi
  # check if pg dump exists
  if [ ! -e ${pgbin}/pg_dump ]; then
    MSG="job id ${jobid} - job can not run - pg_dump not found, check PGBINxx paths in config file ${CONFIGFILE}"
    log "ERROR" "${MSG}"
    ERRORS=1
  else
  # Cluster schema dump
    STARTTIME=`date '+%s'`
    log "INFO" "job id ${jobid} - ${butype} dump started with options ${dumpoptions}"
    if [[ "${butype}" =~ ^CLUSTER ]]; then
      ext=".sql.gz"
      snaplog "INFO" "dumping - ${pgbin}/pg_dumpall ${pgsqlhost} -p ${pgsqlport} -U ${pgsqlsuperuser} -l ${dbname} ${schemaonly} -f ${jobdir}/${buname}${ext} -v"
      ${pgbin}/pg_dumpall ${pgsqlhost} -p ${pgsqlport} -U ${pgsqlsuperuser} -l ${dbname} ${schemaonly} -f /dev/stdout -v 2>> ${jobdir}/${buname}.log | gzip -c > ${jobdir}/${buname}${ext}
      if [ "$?" == "0" ]; then
        log "INFO" "job id ${jobid} - finished pg_dumpall (cluster schema) for ${buname}${ext}"
      else
        MSG="job id ${jobid} - errors occured during pg_dumpall (cluster schema) for ${buname}${ext}"
        log "ERROR" "${MSG}"
        ERRORS=1
      fi
    elif  [ "${butype}" == "SCRIPT" ]; then
    # Script dumps (custom script)
      local scriptname=`echo "${dumpoptions}" | cut -d ' ' -f 1`
      preparescriptrun "${scriptname}"
      if [ ! -e "${SCRIPTDIR}/${scriptname}" ]; then
        snaplog "ERROR" "script not found: ${SCRIPTDIR}/${scriptname}"
        cleanupexit 1
      else
        snaplog "INFO" "script started: ${SCRIPTDIR}/${dumpoptions}"
        # Separate script name from its own options
        local scriptoptions=`echo "${dumpoptions}" | awk '{ print substr($0, length($1)+1) }'`
        ${SCRIPTDIR}/${scriptname} ${TEMPDIR}/$$.${JOBID}.pgsnap_dump_scriptinit.temp >> ${jobdir}/${buname}.log
        local ec=$?
        if [ "$ec" != "0" ]; then
          MSG="job id ${jobid} - script exited with code ${ec} for ${jobdir}/${buname}"
i         log "ERROR" "${MSG}"
          ERRORS=1
        fi
      fi
    else
    # Database specific dumps
      # Tablespaces, users
      ${pgbin}/pg_dumpall ${pgsqlhost} -p ${pgsqlport} -U ${pgsqlsuperuser} -l ${dbname} --globals-only -f ${jobdir}/${buname}.cluster_globals.sql -v 2>> ${jobdir}/${buname}.log
      if [ "$?" != "0" ]; then
        MSG="job id ${jobid} - errors occured during pg_dumpall (globals): ${buname}"
        log "ERROR" "${MSG}"
        ERRORS=1
      fi
      # Database settings
      dumpdbsettings "${dbname}" "${jobdir}/${buname}.database_settings.list"
      # Connection rights
      dumpglobalacls "${dbname}" "${jobdir}/${buname}.database_acl.sql"
      # Dump (possibly schema-only, that's included in the dump options)
      snaplog "INFO" "dumping - ${pgbin}/pg_dump ${pgsqlhost} -p ${pgsqlport} -U ${pgsqlsuperuser} ${dumpoptions} ${schemaonly} -Fd -f ${jobdir}/${buname} -v ${dbname}"
      ${pgbin}/pg_dump ${pgsqlhost} -p ${pgsqlport} -U ${pgsqlsuperuser} ${dumpoptions} ${schemaonly} -Fd -f ${jobdir}/${buname} -v ${dbname} 2>>  ${jobdir}/${buname}.log
      if [ "$?" != "0" ]; then
        MSG="job id ${jobid} - errors occured during pg_dump: ${buname}"
        log "ERROR" "${MSG}"
        ERRORS=1
      fi
      # Extract schema sql, as a quick test of the dump
      ${pgbin}/pg_restore --schema-only -f /dev/null -v ${jobdir}/${buname} 2>>  ${jobdir}/${buname}.log
      if [ "$?" != "0" ]; then
        MSG="job id ${jobid} - errors occured during schema creation from dump: ${buname}"
        log "ERROR" "${MSG}"
        ERRORS=1
      fi
    fi
    # Size on disk
    SIZEONDISK=`du -k ${jobdir}/${buname}${ext} | awk '{print $1 * 1024}'`
    # Db Size
    DBSIZE=$(getdbsize "${pgsqlhost}" ${pgsqlport} "${pgsqlsuperuser}" "${dbname}" "${schema}")
    if [ "${SIZEONDISK}" == "0" ]; then
      ERRORS=1
    fi
    ENDTIME=`date '+%s'`
    let "ELAPSEDTIME = $ENDTIME - $STARTTIME"
    log "INFO" "job id ${jobid} - dump ended - elapsed time ${ELAPSEDTIME}[s]"
 fi # if pg_dump exists
}

# ================================================================================
# MAIN
# ===============================================================================

# Check if already running, quit if so, exit code 0 (its not an error).
if [ "$(isjobrunning ${JOBID})" == "0" ]; then
  lock ${JOBID} "${0} ${1} ${2} ${3} ${4} ${5}"
else
  cleanupexit 0
fi

# Start message
snaplog "INFO" "init - pgsql_instance.job: [`basename ${PGSQLID}`].[${JOBID}]"

# Get pgsql instance status (the instance that contains the db to be backed up)
INSTANCESTATUS=$(getinstancestatus ${PGSQLID})

# Display information
if [ "$VERBOSITY" == "VERBOSE" ]; then
  echo ""
  echo "+--------------------+"
  echo "| pgsnap_dump        |"
  echo "+--------------------+"
  echo ""
  echo "Config file:           ${CONFIGFILE}"
  echo ""
  echo "PgSnapman worker fqdn: ${FQDN}"
  echo "PgSnapman worker id:   ${BUWORKERID}"
  echo "PgSnapman log:         ${PGSNAPMANLOG}"
  echo ""
  echo "Global pgsnapman catalog db"
  echo "  db:   ${PGSCDB}"
  echo "  host: ${PGSCHOST}"
  echo "  port: ${PGSCPORT}"
  echo "  user: ${PGSCUSER}"
  echo ""
  echo "Pgsql instance status: ${INSTANCESTATUS}"
  echo ""
fi

# Break out when pgsql instance for this job is halted
if [ "${INSTANCESTATUS}" == "HALTED" ]; then
  snaplog "WARNING" "abandon job - pgsql instance halted [`basename ${PGSQLID}`]"
  cleanupexit 0
fi

# Get all required job info
# -------------------------
if [ "${STARTTYPE}" == "CRON" ]; then
  jobfile=${DUMPSNAP}/${PGSQLID}/dumpjobs.list
else
  jobfile=${TEMPDIR}/${SINGLERUNPID}.0.singledumpjobs.list
fi
snaplog "INFO" "configuration - reading job info from ${jobfile} [$JOBID]"
row="$(findrow ${jobfile} ${2})"
# Check if we have the required job info, otherwise stop
if [ "${row}" == "" ]; then
  talk "ERROR configuration - pgsql_instance.job [`basename ${PGSQLID}`].[${JOBID}] not found in ${STARTTYPE} job list"
  snaplog "ERROR" "configuration - pgsql_instance.job [`basename ${PGSQLID}`].[${JOBID}] not found in ${STARTTYPE} job list"
  cleanupexit 1
fi
# Read job data
jobid=$(getfieldvalue "${row}" 1)
# SKIP BUWORKERID (if needed in case of a SINGLE job it would have been obtained from the catalog server)
pgsqlid=$(getfieldvalue "${row}" 3)
pgsqlfqdn=$(getfieldvalue "${row}" 16)
pgsqlport=$(getfieldvalue "${row}" 17)
pgsqlsuperuser=$(getfieldvalue "${row}" 18)
dbname=$(getfieldvalue "${row}" 4)
butype=$(getfieldvalue "${row}" 5)
schema=$(getfieldvalue "${row}" 6)
cron=$(getfieldvalue "${row}" 7)
comment=$(getfieldvalue "${row}" 12)
jobstatus=$(getfieldvalue "${row}" 13)
jobtype=$(getfieldvalue "${row}" 14)
restorejobid=$(getfieldvalue "${row}" 19)
dumpoptions=$(getfieldvalue "${row}" 20)
# Set schema_part for the dump name, NOTE special exception:
# when butype is set to SCRIPT, we want to ignore the schema field.
if [ "${schema}" == "*" ] || [ "${butype}" == "SCRIPT" ]; then
  schema_part=""
else
  dumpoptions="--schema=\"${schema}\" ${dumpoptions}"
  schema_part=".${schema}"
fi

# Resolve host, keep original name for logging
pgsqlhost=$(resolvepghost ${pgsqlfqdn})

# In case of JOB type SINGLE (NOT STARTUP TYPE!), the status MUST be read from the server
if [ "${jobtype}" == "SINGLE" ]; then
  # get status from server
  getjobstatus "DUMP" "${JOBID}"
  if [ $ERRCODE -eq 0 ]; then
    jobstatus=${RETVAL}
  else
    MSG="abort - job status for SINGLE job could not be obtained from server"
    snaplog "ERROR" "${MSG}"
    talk "${MSG}"
    cleanupexit $ERRCODE
  fi
fi

# Display a lot of data in verbose mode
if  [ "$VERBOSITY" == "VERBOSE" ]; then
  echo "PgSnapDump job data configuration"
  echo "job file:       ${jobfile}"
  echo "jobid:          ${jobid}"
  echo "pg id:          ${pgsqlid}"
  echo "pg host:        ${pgsqlfqdn}"
  echo "pg port:        ${pgsqlport}"
  echo "pg superuser:   ${pgsqlsuperuser}"
  echo "dbname:         ${dbname}"
  echo "butype:         ${butype}"
  echo "schema:         ${schema}"
  echo "cron:           ${cron}"
  echo "cron:           ${comment}"
  echo -n "job status:     ${jobstatus}"
  if [ "${jobtype}" == "SINGLE" ]; then echo -n " (SINGLE job: retrieved from server)"; fi
  echo -e "\njob type:       ${jobtype}"
  echo "dump options:   ${dumpoptions}"
  echo "restore job id: ${restorejobid}"
  echo ""
fi
# -----------------------------------------------

# Quit when status is halted and job type is single
if [ "${jobtype}" == "SINGLE" ] && [ "${jobstatus}" == "HALTED" ]; then
  MSG="Aborted as job has already run"
  talk "${MSG}"
  cleanupexit 0
fi

# Set job status to started in case of a SINGLE run job (job TYPE, not STARTUP type)
# Both on cron and as single job
# This MUST succeed in order to proceed (otherwise risk of running job over and over again)
ERRCODE=0
if [ "${jobtype}" == "SINGLE" ]; then
  # sets errcode
  setjobstatus "DUMP" ${JOBID} "HALTED"
fi
if [ $ERRCODE -eq 0 ]; then
  # Start the dumpjob
  startpgsnapjob ${PGSQLID} ${JOBID}
else
  MSG="Aborted as job status could not be set, indicates a catalog database error"
  snaplog "ERROR" "${MSG}"
  cleanupexit 1
fi

# Process the results
if [ "$VERBOSITY" == "VERBOSE" ]; then
  echo "Dump location:   ${jobdir}"
  echo "Dump name:       ${buname}${ext}"
  echo "General job log: ${TOOL_LOG}"
  echo "pg_dump log:     ${jobdir}/${buname}.log"
  echo ""
fi

# File format for database import (using COPY FROM STDIN)
# columns: pgsnap_job_id starttime endtime status bu_name bu_location dbsize dumpsize

# Write status info for log and catalog
if [ "${ERRORS}" == "1" ]; then
  snaplog "ERROR" "finished - pgsql_instance.job [`basename ${PGSQLID}`].[${JOBID}] with errors, check: ${TOOL_LOG}"
  jobresult="FAILED"
  DBSIZE=-1
  SIZEONDISK=-1
else
  # use comment as message
  MSG=${comment}
  snaplog "INFO" "dump completed - pgsql_instance.job [`basename ${PGSQLID}`].[${JOBID}]"
  jobresult="SUCCESS"
fi

# Write catalog data file for upload, symlink in central upload dir
copylog=${jobdir}/${buname}.catalog.dat
preparecatalogdata "${2}\t${INITIMESTAMP}\t`date +'%Y%m%dT%H%M%S%z'`\t${jobresult}\t${buname}${ext}\t${jobdir}\t${DBSIZE}\t${SIZEONDISK}\t${dbname}\t${schema}\t${butype}\t${ext}\t${pgversion}\t${BUWORKERID}\t${MSG}\t${pgsqlfqdn}\t${pgsqlport}" "${copylog}" "NEW" "UPLOAD"

# Next procedures, set exit code
if [ "${jobresult}" == "FAILED" ]; then
  cleanupexit 1
else
  # Start dedup for a FULL backup
  if [ "${butype}" == "FULL" ]; then
    ${SCRIPTPATH}/pgsnap_dedup ${jobdir}/${buname} SILENT ${jobid}
    snaplog "INFO" "finished deduplication of table data"
  fi
  # Start a restore job if requested, put it in the background, the dump is ready now
  if [ "${restorejobid}" != "" ]; then
    # Could be multiple restore jobs!
    OIFS="${IFS}"; IFS=","
    for rid in ${restorejobid}; do
      log "INFO" "job id ${jobid} - triggered start of restore job ${rid}"
      ${SCRIPTPATH}/pgsnap_restore ${rid} ${jobdir}/${buname} SILENT TRIGGER &
    done
    IFS="${OIFS}"
  fi
  snaplog "INFO" "finished dump process"
  cleanupexit 0
fi

# EOF

