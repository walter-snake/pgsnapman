#!/bin/bash

# pgsnap_dump: starts a single postgres dump operation
#                     must run on a backup node, with passwordless superuser access (.pgpass does the job)
#
# $1 pgsql instance id <pginstanceid_pgdns_pgport>, resolves to snapshot/pgsqlinstance directory (must contain the job list)
# $2 dump job id
# $3 verbosity [VERBOSE|SILENT]
# $4 startup type [CRON|SINGLE]
#      CRON will use cached job information in <snapshots>/<serverinstance>/dumpjobs.list
#      SINGLE use temp cached information in <temp>/$5.0.singledumpjobs.list
# $5 single run process id (for linking temp job info cache)
#
# Actual backups are usually started by cron on this machine, but you may invoke it manually. This will override
# HALTED jobs in CRON mode.

# ======================================
# Initialization
# ======================================
PGSQLID=$1
JOBID=$2
VERBOSITY=$3
STARTTYPE=$4
SINGLERUNPID=$5

# Get the script directory (must do this first)
SOURCE="${BASH_SOURCE[0]}"
while [ -h "$SOURCE" ]; do # resolve $SOURCE until the file is no longer a symlink
  DIR="$( cd -P "$( dirname "$SOURCE" )" && pwd )"
  SOURCE="$(readlink "$SOURCE")"
  [[ $SOURCE != /* ]] && SOURCE="$DIR/$SOURCE" # if $SOURCE was a relative symlink, we need to resolve it relative to the path where the symlink file was located
done
SCRIPTPATH="$( cd -P "$( dirname "$SOURCE" )" && pwd )"

# Check cmd args
if [ "${STARTTYPE}" == "" ]; then
  echo "ERROR pgsnap_dump no startup type provided"
  cleanupexit 1
fi
if [ "${STARTTYPE}" == "SINGLE" ] && [ "${SINGLERUNPID}" == "" ]; then
  echo "ERROR pgsnap_dump needs a process id in SINGLE mode"
  cleanupexit 1
fi

# Catalog database needed? YES when running in SINGLE MODE, we must check if a job may start (job status ACTIVE)
if [ "${STARTTYPE}" == "CRON" ]; then
  PGSCDB_REQUIRED=NO
else
  PGSCDB_REQUIRED=YES
fi

# Load functions, will also perform initialization operations
. ${SCRIPTPATH}/pgsnap_lib

# ============================
# Functions
# ============================

# Prepare for starting a custom script
# Writes out all current variables to a temp file
# Downloads required script
# $1 script name
function preparescriptrun {
  echo "JOBID=${jobid}" > ${TEMPDIR}/$$.${JOBID}.pgsnap_dump_scriptinit.temp
  echo "INITIMESTAMP=${INITIMESTAMP}" >> ${TEMPDIR}/$$.${JOBID}.pgsnap_dump_scriptinit.temp
  echo "VERBOSITY=${VERBOSITY}" >> ${TEMPDIR}/$$.${JOBID}.pgsnap_dump_scriptinit.temp
  echo "STARTTYPE=${STARTTYPE}" >> ${TEMPDIR}/$$.${JOBID}.pgsnap_dump_scriptinit.temp
  echo "SINGLERUNPID=${SINGLERUNPID}" >> ${TEMPDIR}/$$.${JOBID}.pgsnap_dump_scriptinit.temp
  echo "JOBDIR=${jobdir}" >> ${TEMPDIR}/$$.${JOBID}.pgsnap_dump_scriptinit.temp
  echo "BUNAME=${buname}" >> ${TEMPDIR}/$$.${JOBID}.pgsnap_dump_scriptinit.temp
  echo "PGSQLHOST=\"${pgsqlhost}\"" >> ${TEMPDIR}/$$.${JOBID}.pgsnap_dump_scriptinit.temp
  echo "PGSQLPORT=${pgsqlport}" >> ${TEMPDIR}/$$.${JOBID}.pgsnap_dump_scriptinit.temp
  echo "PGSQLSUPERUSER=${pgsqlsuperuser}" >> ${TEMPDIR}/$$.${JOBID}.pgsnap_dump_scriptinit.temp
  echo "PGSQLDBNAME=${dbname}" >> ${TEMPDIR}/$$.${JOBID}.pgsnap_dump_scriptinit.temp
  echo "SCHEMA=${schema}" >> ${TEMPDIR}/$$.${JOBID}.pgsnap_dump_scriptinit.temp
  echo "ROOTDIR=${ROOTDIR}" >> ${TEMPDIR}/$$.${JOBID}.pgsnap_dump_scriptinit.temp
  echo "LOGDIR=${LOGDIR}" >> ${TEMPDIR}/$$.${JOBID}.pgsnap_dump_scriptinit.temp
  echo "TEMPDIR=${TEMPDIR}" >> ${TEMPDIR}/$$.${JOBID}.pgsnap_dump_scriptinit.temp
  echo "UPLOADDIR=${UPLOADDIR}" >> ${TEMPDIR}/$$.${JOBID}.pgsnap_dump_scriptinit.temp
  echo "PGBIN=${pgbin}" >> ${TEMPDIR}/$$.${JOBID}.pgsnap_dump_scriptinit.temp
  echo "PGSNAPMANLOG=${PGSNAPMANLOG}" >> ${TEMPDIR}/$$.${JOBID}.pgsnap_dump_scriptinit.temp
  echo "TOOL_LOG=${TOOL_LOG}" >> ${TEMPDIR}/$$.${JOBID}.pgsnap_dump_scriptinit.temp
  echo "BUWORKERID=${BUWORKERID}" >> ${TEMPDIR}/$$.${JOBID}.pgsnap_dump_scriptinit.temp
  if [ "${jobdir}" == "" ] || [ "${buname}" == "" ]; then
    ERRCODE=1
  else   
    mkdir ${jobdir}/${buname}
    # Download script
    if [ ! -e ${SCRIPTDIR}/${1} ]; then
      getscript "${1}"
    fi
  fi
}

# Get the global ACL's from the database, and convert them to sql format
# $1 store in file (full path)
function dumpglobalacls {
  local line
  # The nice way, using a function (but that's not present in every database)
  # local sql="select * from get_globalacl('${1}') as (datname name, acl_role text, acl_rights text);"
  # Ugly but acceptable: we checked for ; in the name, but probably nothing will work with a weird database name (you'll have to connect in the first place)
  local sql="with acls as (select datname, split_part(acl, '=', 1) as acl_role \
  , (regexp_split_to_array(acl, '[=/]'))[2] as acl_rights \
from (select datname, unnest(datacl)::text as acl \
  from pg_database) a \
) \
select datname,acl_role,acl_rights from acls \
where datname = '${dbname}'"
  dbquerytofile "${pgsqlhost}" "${pgsqlport}" "${pgsqlsuperuser}" "${dbname}" "${sql}" "${1}"
}

# Dump the database settings (raw table format, | separated)
# $1 export file
function dumpdbsettings {
  local sql="SELECT d.datname, pg_catalog.pg_get_userbyid(d.datdba) as owner, pg_catalog.pg_encoding_to_char(d.encoding) as encoding, \
    d.datcollate as collate, d.datctype as ctype, pg_catalog.array_to_string(d.datacl, ',') AS acl_rights \
    , spcname as dattablespace \
    FROM pg_catalog.pg_database d \
    LEFT JOIN (SELECT oid, spcname FROM pg_catalog.pg_tablespace WHERE spcname NOT LIKE 'pg_default') t \
      ON d.dattablespace = t.oid \
    WHERE d.datname = '${dbname}';"
  dbquerytofile "${pgsqlhost}" "${pgsqlport}" "${pgsqlsuperuser}" "${dbname}" "${sql}" "${1}"
}

# Check if we should make a backup, taking into account dump on demand settings
# $1 database name
function isbackuprequired {
  local sql=""
  local val=""
  # When not using on demand or in SINGLE mode, always return YES (thus: dump is schedule driven)
  if [ "${ENABLE_DUMP_ON_DEMAND}" != "YES" ] || [ "${STARTTYPE}" == "SINGLE" ]; then
    echo "YES"
  else
    # Check the metainfo key, and we act a bit blunt: if not found just create table and schema and insert key (in normal SCHEDULE mode)
    val=$( getmetainfo "${pgsqlhost}" "${pgsqlport}" "${pgsqlsuperuser}" "${1}" "TAKE_BACKUP" )
    if [ "${val}" == "" ]; then
      createmetainfo "${pgsqlhost}" "${pgsqlport}" "${pgsqlsuperuser}" "${1}"
      setmetainfo "${pgsqlhost}" "${pgsqlport}" "${pgsqlsuperuser}" "${1}" "TAKE_BACKUP" "SCHEDULE"
    fi
    val=$( getmetainfo "${pgsqlhost}" "${pgsqlport}" "${pgsqlsuperuser}" "${1}" "TAKE_BACKUP" )
    if [ "${val}" != "NO" ]; then
      echo "YES"
    else
      echo "NO"
    fi
  fi
}

# Start a specific dump job 
# $1 path to postgres instance snapshot root directory
# $2 id of job to start
function startpgsnapjob {
  # Determine job type, as naming schema's differ, also set --schema-only flag if required
  if [ "${butype}" == "CLUSTER_SCHEMA" ]; then
    jobdir=${DUMPSNAP}/${1}/${jobid}_cluster.schema
  elif [ "${butype}" == "CLUSTER" ]; then
    jobdir=${DUMPSNAP}/${1}/${jobid}_cluster.full
  else
    jobdir=${DUMPSNAP}/${1}/${jobid}_${dbname}${schema_part}
  fi
  if [[ "${butype}" =~ .*SCHEMA ]]; then
    schemaonly="--schema-only"
  fi 
  mkdir -p ${jobdir}

  # Set logfile
  TOOL_LOG=${jobdir}.log

  # Get filename, postgres version
  buname=${jobid}_${dbname}${schema_part}_${butype}_`date '+%Y%m%dT%H%M%S'`
  log "INFO" "job id ${jobid} - ${jobtype} backup ${buname}"
  # Check postgres server version and binary availability 
  pgversion=$(getpgversion "${pgsqlhost}" "${pgsqlport}" "${pgsqlsuperuser}" "${dbname}")
  if [ "${pgversion}" == "" ]; then
    MSG="job id ${jobid} - could not connect to postgres instance: ${pgsqlsuperuser}@${pgsqlfqdn}:${pgsqlport}/${dbname}"
    log "ERROR" "${MSG}"
    ERRORS=1
    return
  else
    eval pgbinvar=PGBIN${pgversion}
    pgbin=${!pgbinvar}
  fi

  # check if pg dump exists
  if [ ! -e ${pgbin}/pg_dump ]; then
    MSG="job id ${jobid} - job can not run - pg_dump not found, check PGBINxx paths in config file ${CONFIGFILE}"
    log "ERROR" "${MSG}"
    ERRORS=1
  else
  # Cluster schema dump
    STARTTIME=`date '+%s'`
    log "INFO" "job id ${jobid} - ${butype} dump started with options ${dumpoptions}"
    if [[ "${butype}" =~ ^CLUSTER ]]; then
      ext=".sql.gz"
      snaplog "INFO" "dumping - ${pgbin}/pg_dumpall ${pgsqlhost} -p ${pgsqlport} -U ${pgsqlsuperuser} -l ${dbname} ${schemaonly} -f ${jobdir}/${buname}${ext} -v"
      ${pgbin}/pg_dumpall ${pgsqlhost} -p ${pgsqlport} -U ${pgsqlsuperuser} -l ${dbname} ${schemaonly} -f /dev/stdout -v 2>> ${jobdir}/${buname}.log | gzip -c > ${jobdir}/${buname}${ext}
      if [ "$?" == "0" ]; then
        log "INFO" "job id ${jobid} - finished pg_dumpall (cluster schema) for ${buname}${ext}"
      else
        MSG="job id ${jobid} - errors occured during pg_dumpall (cluster schema) for ${buname}${ext}"
        log "ERROR" "${MSG}"
        ERRORS=1
      fi
    elif  [ "${butype}" == "SCRIPT" ]; then
    # Script dumps (custom script)
      local scriptname=`echo "${dumpoptions}" | cut -d ' ' -f 1`
      preparescriptrun "${scriptname}"
      if [ ! -e "${SCRIPTDIR}/${scriptname}" ]; then
        snaplog "ERROR" "script not found: ${SCRIPTDIR}/${scriptname}"
        cleanupexit 1
      else
        snaplog "INFO" "script started: ${SCRIPTDIR}/${dumpoptions}"
        # Separate script name from its own options
        local scriptoptions=`echo "${dumpoptions}" | awk '{ print substr($0, length($1)+1) }'`
        ${SCRIPTDIR}/${scriptname} ${TEMPDIR}/$$.${JOBID}.pgsnap_dump_scriptinit.temp >> ${jobdir}/${buname}.log
        local ec=$?
        if [ "$ec" != "0" ]; then
          MSG="job id ${jobid} - script exited with code ${ec} for ${jobdir}/${buname}"
i         log "ERROR" "${MSG}"
          ERRORS=1
        fi
      fi
    else
    # Database specific dumps
      # Tablespaces, users
      ${pgbin}/pg_dumpall ${pgsqlhost} -p ${pgsqlport} -U ${pgsqlsuperuser} -l ${dbname} --globals-only -f ${jobdir}/${buname}.cluster_globals.sql -v 2>> ${jobdir}/${buname}.log
      if [ "$?" != "0" ]; then
        MSG="job id ${jobid} - errors occured during pg_dumpall (globals): ${buname}"
        log "ERROR" "${MSG}"
        ERRORS=1
      fi
      # Database settings
      dumpdbsettings "${jobdir}/${buname}.database_settings.list"
      # Connection rights
      dumpglobalacls "${jobdir}/${buname}.database_acl.list"
      # Dump (possibly schema-only, that's included in the dump options)
      snaplog "INFO" "dumping - ${pgbin}/pg_dump ${pgsqlhost} -p ${pgsqlport} -U ${pgsqlsuperuser} ${dumpoptions} ${schemaonly} -Fd -f ${jobdir}/${buname} -v ${dbname}"
      ${pgbin}/pg_dump ${pgsqlhost} -p ${pgsqlport} -U ${pgsqlsuperuser} ${dumpoptions} ${schemaonly} -Fd -f ${jobdir}/${buname} -v ${dbname} 2>>  ${jobdir}/${buname}.log
      if [ "$?" != "0" ]; then
        MSG="job id ${jobid} - errors occured during pg_dump: ${buname}"
        log "ERROR" "${MSG}"
        ERRORS=1
      fi
      # Extract schema sql, as a quick test of the dump
      ${pgbin}/pg_restore --schema-only -f /dev/null -v ${jobdir}/${buname} 2>>  ${jobdir}/${buname}.log
      if [ "$?" != "0" ]; then
        MSG="job id ${jobid} - stage 1 verify failed: could not extract schema from dump"
        log "ERROR" "${MSG}"
        ERRORS=1
      fi
    fi
    # Size on disk
    SIZEONDISK=`du -k ${jobdir}/${buname}${ext} | awk '{print $1 * 1024}'`
    # Db Size
    DBSIZE=$(getdbsize "${pgsqlhost}" ${pgsqlport} "${pgsqlsuperuser}" "${dbname}" "${schema}")
    if [ "${SIZEONDISK}" == "0" ]; then
      ERRORS=1
    fi
    ENDTIME=`date '+%s'`
    let "ELAPSEDTIME = $ENDTIME - $STARTTIME"
    log "INFO" "job id ${jobid} - dump ended - elapsed time ${ELAPSEDTIME}[s]"
 fi # if pg_dump exists
}

# ================================================================================
# MAIN
# ===============================================================================

# Check if already running, quit if so, exit code 0 (its not an error).
if [ "$(isjobrunning ${JOBID})" == "0" ]; then
  lock ${JOBID} "${0} ${1} ${2} ${3} ${4} ${5}"
else
  cleanupexit 0
fi

# Start message
snaplog "INFO" "init - pgsql_instance.job: [$( basename ${PGSQLID} )].[${JOBID}]"

# Get pgsql instance status (the instance that contains the db to be backed up)
INSTANCESTATUS=$(getinstancestatus ${PGSQLID})

# Display information
if [ "$VERBOSITY" == "VERBOSE" ]; then
  echo ""
  echo "+--------------------+"
  echo "| pgsnap_dump        |"
  echo "+--------------------+"
  echo ""
  echo "Config file:           ${CONFIGFILE}"
  echo ""
  echo "PgSnapman worker fqdn: ${FQDN}"
  echo "PgSnapman worker id:   ${BUWORKERID}"
  echo "PgSnapman log:         ${PGSNAPMANLOG}"
  echo ""
  echo "Global pgsnapman catalog db"
  echo "  db:   ${PGSCDB}"
  echo "  host: ${PGSCHOST}"
  echo "  port: ${PGSCPORT}"
  echo "  user: ${PGSCUSER}"
  echo ""
  echo "Pgsql instance status: ${INSTANCESTATUS}"
  echo ""
fi

# Break out when pgsql instance for this job is halted
if [ "${INSTANCESTATUS}" == "HALTED" ]; then
  snaplog "WARNING" "abandon job - pgsql instance halted [`basename ${PGSQLID}`]"
  cleanupexit 0
fi

# Get all required job info
# -------------------------
if [ "${STARTTYPE}" == "CRON" ]; then
  jobfile=${DUMPSNAP}/${PGSQLID}/dumpjobs.list
else
  jobfile=${TEMPDIR}/${SINGLERUNPID}.0.singledumpjobs.list
fi
snaplog "INFO" "configuration - reading job info from ${jobfile} [$JOBID]"
row="$(findrow ${jobfile} ${2})"
# Check if we have the required job info, otherwise stop
if [ "${row}" == "" ]; then
  talk "ERROR configuration - pgsql_instance.job [`basename ${PGSQLID}`].[${JOBID}] not found in ${STARTTYPE} job list"
  snaplog "ERROR" "configuration - pgsql_instance.job [`basename ${PGSQLID}`].[${JOBID}] not found in ${STARTTYPE} job list"
  cleanupexit 1
fi
# Read job data
jobid=$(getfieldvalue "${row}" 1)
# SKIP BUWORKERID (if needed in case of a SINGLE job it would have been obtained from the catalog server)
pgsqlid=$(getfieldvalue "${row}" 3)
pgsqlfqdn=$(getfieldvalue "${row}" 16)
pgsqlport=$(getfieldvalue "${row}" 17)
pgsqlsuperuser=$(getfieldvalue "${row}" 18)
dbname=$(getfieldvalue "${row}" 4)
butype=$(getfieldvalue "${row}" 5)
schema=$(getfieldvalue "${row}" 6)
cron=$(getfieldvalue "${row}" 7)
comment=$(getfieldvalue "${row}" 12)
jobstatus=$(getfieldvalue "${row}" 13)
jobtype=$(getfieldvalue "${row}" 14)
restorejobid=$(getfieldvalue "${row}" 19)
dumpoptions=$(getfieldvalue "${row}" 20)
# Set schema_part for the dump name, NOTE special exception:
# when butype is set to SCRIPT, we want to ignore the schema field.
if [ "${schema}" == "*" ] || [ "${butype}" == "SCRIPT" ]; then
  schema_part=""
else
  dumpoptions="--schema=\"${schema}\" ${dumpoptions}"
  schema_part=".${schema}"
fi

# Resolve host, keep original name for logging
pgsqlhost=$(resolvepghost ${pgsqlfqdn})

# In case of JOB type SINGLE (NOT STARTUP TYPE!), the status MUST be read from the server
if [ "${jobtype}" == "SINGLE" ]; then
  # get status from server
  getjobstatus "DUMP" "${JOBID}"
  if [ $ERRCODE -eq 0 ]; then
    jobstatus=${RETVAL}
  else
    MSG="abort - job status for SINGLE job could not be obtained from server"
    snaplog "ERROR" "${MSG}"
    talk "${MSG}"
    cleanupexit $ERRCODE
  fi
fi

# Display a lot of data in verbose mode
if  [ "$VERBOSITY" == "VERBOSE" ]; then
  echo "PgSnapDump job data configuration"
  echo "job file:       ${jobfile}"
  echo "jobid:          ${jobid}"
  echo "pg id:          ${pgsqlid}"
  echo "pg host:        ${pgsqlfqdn}"
  echo "pg port:        ${pgsqlport}"
  echo "pg superuser:   ${pgsqlsuperuser}"
  echo "dbname:         ${dbname}"
  echo "butype:         ${butype}"
  echo "schema:         ${schema}"
  echo "cron:           ${cron}"
  echo "cron:           ${comment}"
  echo -n "job status:     ${jobstatus}"
  if [ "${jobtype}" == "SINGLE" ]; then echo -n " (SINGLE job: retrieved from server)"; fi
  echo -e "\njob type:       ${jobtype}"
  echo "dump options:   ${dumpoptions}"
  echo "restore job id: ${restorejobid}"
  echo ""
fi
# -----------------------------------------------

# Quit when status is halted and job type is single
if [ "${jobtype}" == "SINGLE" ] && [ "${jobstatus}" == "HALTED" ]; then
  MSG="Aborted as job has already run"
  talk "${MSG}"
  cleanupexit 0
fi

# Set job status to started in case of a SINGLE run job (job TYPE, not STARTUP type)
# Both on cron and as single job
# This MUST succeed in order to proceed (otherwise risk of running job over and over again)
ERRCODE=0
ERRORS=0
if [ "${jobtype}" == "SINGLE" ]; then
  # sets errcode
  setjobstatus "DUMP" ${JOBID} "HALTED"
fi
if [ $ERRCODE -eq 0 ]; then
  # Check if backup is required
  if [ "$( isbackuprequired "${dbname}" )" == "YES" ]; then
    # Start the dumpjob
    startpgsnapjob ${PGSQLID} ${JOBID}
  else
    MSG="dump job id ${JOBID} aborted by DUMP_ON_DEMAND status in database (TAKE_BACKUP=NO)"
    preparecatalogdata "`date '+%Y%m%dT%H%M%S%z'`\tINFO\tpgsnap_dump\t${MSG}\tDUMP\t${JOBID}\t${BUWORKERID}" "${UPLOADDIR}/$$.pgsnap_dump.message.dat" "NEW" "" 
    snaplog "INFO" "${MSG}"
    cleanupexit 1
  fi
else
  MSG="Aborted as job status could not be set, indicates a catalog database error"
  snaplog "ERROR" "${MSG}"
  cleanupexit 1
fi

# Process the results
if [ "$VERBOSITY" == "VERBOSE" ]; then
  echo "Dump location:   ${jobdir}"
  echo "Dump name:       ${buname}${ext}"
  echo "General job log: ${TOOL_LOG}"
  echo "pg_dump log:     ${jobdir}/${buname}.log"
  echo ""
fi

# Write status info for log and catalog
if [ "${ERRORS}" == "1" ]; then
  snaplog "ERROR" "finished - pgsql_instance.job [`basename ${PGSQLID}`].[${JOBID}] with errors, check: ${TOOL_LOG}"
  jobresult="FAILED"
  DBSIZE=-1
  SIZEONDISK=-1
else
  # use comment as message
  MSG=${comment}
  snaplog "INFO" "dump completed - pgsql_instance.job [`basename ${PGSQLID}`].[${JOBID}]"
  jobresult="SUCCESS"
fi

# Write catalog data file for upload, symlink in central upload dir
copylog=${jobdir}/${buname}.catalog.dat
preparecatalogdata "${2}\t${INITIMESTAMP}\t`date +'%Y%m%dT%H%M%S%z'`\t${jobresult}\t${buname}${ext}\t${jobdir}\t${DBSIZE}\t${SIZEONDISK}\t${dbname}\t${schema}\t${butype}\t${ext}\t${pgversion}\t${BUWORKERID}\t${MSG}\t${pgsqlfqdn}\t${pgsqlport}" "${copylog}" "NEW" "UPLOAD"

# Next procedures, set exit code
if [ "${jobresult}" == "FAILED" ]; then
  cleanupexit 1
else
  if [ "$( getmetainfo "${pgsqlhost}" ${pgsqlport} ${pgsqlsuperuser} "${dbname}" "TAKE_BACKUP" )" == "YES" ]; then
    setmetainfo "${pgsqlhost}" ${pgsqlport} ${pgsqlsuperuser} "${dbname}" "TAKE_BACKUP" "NO"
  fi
  # Start dedup for a FULL backup
  if [ "${butype}" == "FULL" ]; then
    ${SCRIPTPATH}/pgsnap_dedup ${jobdir}/${buname} SILENT ${jobid}
    snaplog "INFO" "finished deduplication of table data"
  fi
  # Start a restore job if requested, put it in the background, the dump is ready now
  if [ "${restorejobid}" != "" ]; then
    ${SCRIPTPATH}/pgsnap_trigger SILENT ${JOBID} ${restorejobid} ${jobdir}/${buname} &
  fi
  snaplog "INFO" "finished dump process"
  cleanupexit 0
fi

# EOF

