#!/bin/bash

# pgsnap_cacheconfig: reads postgres server instances to backup from central configuration database
#                     must run on a backup worker, with passwordless (.pgpass does the job) to the
#                     central configuration database
# Actions taken:
#   1 connects to central configuration database
#   2 reads which pg instances should be backed up by this host
#   3 creates directory structure and writes connection info for every pg instance
#   4 reads jobs from central configuration database, creates directory structure per database and write cron job file
#
# You may refresh the configuration as often as you like, the actual backup proces runs entirely locally (does not need
# access to the central configuration database). 
# 
# Actual backups will be started by cron on this machine, each backup job is one cron entry, starts script: pgsnap_dump.

# ======================================
# Initialization
# ======================================
VERBOSITY=$1

# Get the script directory (must do this first)
SOURCE="${BASH_SOURCE[0]}"
while [ -h "$SOURCE" ]; do # resolve $SOURCE until the file is no longer a symlink
  DIR="$( cd -P "$( dirname "$SOURCE" )" && pwd )"
  SOURCE="$(readlink "$SOURCE")"
  [[ $SOURCE != /* ]] && SOURCE="$DIR/$SOURCE" # if $SOURCE was a relative symlink, we need to resolve it relative to the path where the symlink file was located
done
SCRIPTPATH="$( cd -P "$( dirname "$SOURCE" )" && pwd )"

# Load functions, will also perform initialization operations
. ${SCRIPTPATH}/pgsnap_lib

# ======================================
# Function definitions (script specific)
# ======================================

# Retrieves the worker configurations from the catalog database
# $1 backup worker id (numeric)
function getworkerconfig {
  # Get worker configuration
  ${PGSCBIN}/psql ${PGSCHOST} -p ${PGSCPORT} -U ${PGSCUSER} --dbname=${PGSCDB} -F '|' -A -t -c "SELECT id,dns_name,comment, \
    cron_cacheconfig,cron_singlejob,cron_clean,cron_upload
    FROM vw_worker WHERE id = $1" > $ROOTDIR/pgsnap_worker.cron
}

# Retrieves jobs from the configuration database
# $1 postgres host by id
# $2 instance snapshot location (full path)
function getjobs {
  # Get postgres worker information for workers managed by this backup worker (based on hostname) and cache the data
  # Make creating/replacing it an atomic operation, a dump could start while writing this file. 
  ${PGSCBIN}/psql ${PGSCHOST} -p ${PGSCPORT} -U ${PGSCUSER} --dbname=${PGSCDB} -F '|' -A -t -c "SELECT id,pgsnap_worker_id,pgsql_instance_id,dbname,dumptype,dumpschema, \
      cron,keep_daily,keep_weekly,keep_monthly,keep_yearly, \
      comment,status,jobtype, \
      pgsnap_worker_dns_name,pgsql_instance_dns_name,pgsql_instance_port,pgsql_instance_superuser,pgsnap_restorejob_id,dumpoptions \
    FROM vw_dumpjob_worker_instance \
    WHERE pgsnap_worker_id = ${BUWORKERID} AND pgsql_instance_id = ${1} \
      AND jobtype = 'CRON' \
    ORDER BY id" > ${2}/dumpjobs.list.temp
  mv ${2}/dumpjobs.list.temp ${2}/dumpjobs.list
  if [ "$VERBOSITY" == "VERBOSE" ]; then
    echo "Job data for postgres instance id: "$1
    echo "-------------------------------------"
    echo "Snapshot base directory: "$2
    echo "id|pgsnap_worker_id|pgsql_instance_id|dbname|dumptype|dumpschema|cron|keep_daily|keep_weekly|keep_monthly|keep_yearly|comment|jobstatus|jobtype|dumpoptions"
    cat ${2}/dumpjobs.list
    echo ""
  fi
}

# Set up directory structure for database dumps, based on data in the local cache (dumpjobs.list)
# $1 full path to a postgres instance snapshot directory
function preparejobs {
  if  [ "$VERBOSITY" == "VERBOSE" ]; then
    echo "Processing: "$1/dumpjobs.list
  fi
  echo "# pgsnapman pgsnap_dump jobs for "`basename $1` >> ${ROOTDIR}/cron_snapjobs.temp
  while read line; do
    jobid=$(echo "${line}" | cut -d '|' -f 1)
    pgsqlid=$(echo "${line}" | cut -d '|' -f 3)
    pgsqlhost=$(echo "${line}" | cut -d '|' -f 16)
    pgsqlport=$(echo ${line} | cut -d '|' -f 17)
    pgsqlsuperuser=$(echo "${line}" | cut -d '|' -f 18)
    dbname=$(echo "${line}" | cut -d '|' -f 4)
    butype=$(echo "${line}" | cut -d '|' -f 5)
    schema=$(echo "${line}" | cut -d '|' -f 6)
    dumpoptions=$(echo "${line}" | cut -d '|' -f 20)
    cron=$(echo "${line}" | cut -d '|' -f 7)
    jobstatus=$(echo "${line}" | cut -d '|' -f 13)
    jobtype=$(echo "${line}" | cut -d '|' -f 14)
    restorejobid=$(echo "${line}" | cut -d '|' -f 19)
    # Set schema_part for the dump name
    if [ "${schema}" == "*" ]; then
      schema_part=""
    else
      dumpoptions="--schema=${schema} ${dumpoptions}"
      schema_part="."${schema}
    fi

    # Resolve host
    pgsqlhost=$(resolvepghost ${pgsqlhost})
    
    # Display a lot of data in verbose mode
    if  [ "$VERBOSITY" == "VERBOSE" ]; then
      echo "jobid:          ${jobid}"
      echo "pg id:          ${pgsqlid}"
      echo "pg host:        ${pgsqlhost:3}"
      echo "pg port:        ${pgsqlport}"
      echo "pg superuser:   ${pgsqlsuperuser}"
      echo "dbname:         ${dbname}"
      echo "butype:         ${butype}"
      echo "schema:         ${schema}"
      echo "cron:           ${cron}"
      echo "job status:     ${jobstatus}"
      echo "job type:       ${jobtype}"
      echo "dump options:   ${dumpoptions}"
      echo "restore job id: ${restorejobid}"
      echo ""
    fi

    # Create a cron job entry, only if active.
    # If postgres version could not be determined there was an error, but it will emitted as a warning at this stage.
    # The database may be available at the scheduled time, so the dump tool will repeat this test.
    PRE=""
    log "INFO" "job id ${jobid} - ${pgsqlsuperuser}@${pgsqlhost}:${pgsqlport}/${dbname}${schema_part}"
    echo "# INFO pgsnapman job id ${jobid} - ${pgsqlsuperuser}@${pgsqlhost}:${pgsqlport}/${dbname}${schema_part}" >> ${ROOTDIR}/cron_snapjobs.temp 
    # For active jobs, try to verify the database connection and postgresql version
    if [ "${jobstatus}" == "ACTIVE" ]; then
      # Check postgres instance version
      pgversion=$(getpgversion "${pgsqlhost}" ${pgsqlport} ${pgsqlsuperuser} ${dbname})
      if [ "${pgversion}" == "" ]; then
        log "WARNING" "job id ${jobid} - could not connect to postgres instance: ${pgsqlsuperuser}@${pgsqlhost}:${pgsqlport}/${dbname}${schema_part} - pg_dump version can not be verified, leaving job in place"
      else
        # Get pg bin path for this version
        eval pgbinvar=PGBIN${pgversion}
        pgbin=${!pgbinvar}
      fi
      # Verify if pg_dump is available, disable when db connection could be made, version
      # checked and pg_dump not available (this is the only guaranteed problem, databases
      # could become available later).
      if [ "${pgversion}" != "" ]; then
        if [ ! -e ${pgbin}/pg_dump ]; then
          PRE="# "
          log "ERROR" "job id ${jobid} - job is disabled as pg_dump could not be found, check pgbinxx paths in config file ${CONFIGFILE})"
          echo "# ERROR pgsnapman job is disabled as pg_dump could not be found" >> ${ROOTDIR}/cron_snapjobs.temp
        fi
      fi
    fi
    # Emit warning on halted jobs
    if [ "${jobstatus}" == "HALTED" ]; then
      PRE="# "
      log "WARNING" "job id ${jobid} - disabled by user (status HALTED): ${pgsqlsuperuser}@${pgsqlhost}:${pgsqlport}/${dbname}${schema_part}"
      echo "# WARNING pgsnapman job is disabled by user" >> ${ROOTDIR}/cron_snapjobs.temp
    fi
    # Write cron entry
    echo "${PRE}${cron} ${SCRIPTPATH}/pgsnap_dump ${1} ${jobid}" >> ${ROOTDIR}/cron_snapjobs.temp
    log "INFO" "job id ${jobid} - cron entry created: ${PRE}${cron} ${SCRIPTPATH}/pgsnap_dump ${1} ${jobid}"
  done < $1/dumpjobs.list
}

# Retrieves all databases from a postgres server instance, adds all those that are not listed in the dumpjobs table, with defaults
# - Does not depend on worker, all jobs are checked (a databas could purposely be put on another backup worker).
# - SINGLE jobs do count
# $1 = pg instance id
# $2 = pg dns name
# $3 = pg port
# $4 = pg user
function createdumpjobs {
  # Resolve host
  local fqdnhost=${2}
  local pgsqlhost=$(resolvepghost ${2})

  # Find the databases (exclude templates and all database that you cannot connect to)
  local DBS=`psql ${pgsqlhost} -p ${3} -U ${4} --dbname=${MAINTDB} -A -t -c "SELECT datname FROM pg_database WHERE datistemplate = false AND datallowconn = true;"` 2> /dev/null
  if [ "$?" != "0" ]; then
    echo "ERREUR"
    return
  fi

  # Process the database list
  local c=0
  for line in ${DBS}; do
    local HASJOB=`psql ${PGSCHOST} -p ${PGSCPORT} -U ${PGSCUSER} --dbname=${PGSCDB} -A -t -c "SELECT get_hasjob(${1}, '${line}');"`
    if [ "${HASJOB}" != "YES" ]; then
      echo "add database: ${fqdnhost}:${3}/${line}"
      local jobid=`psql ${PGSCHOST} -p ${PGSCPORT} -U ${PGSCUSER} --dbname=${PGSCDB} -A -t -c "SELECT put_dumpjob(${BUWORKERID}, ${1}, '${line}', 'auto added dump job');"`
      echo "new dump job created with id: ${jobid}"
      let "c = c + 1"
    fi
  done
  if [ ${c} -gt 0 ]; then 
    snaplog "INFO" "processing - auto job: added ${c} dump jobs on worker [${BUWORKERID}] for postgres instance [${fqdnhost}:${3}]"
  fi
}

# ================================================================================
# MAIN
# ===============================================================================

# Tool log
TOOL_LOG=${ROOTDIR}/pgsnap_cacheconfig.log 

# Set the worker id for this worker instance
BUWORKERID=$(getworkerid "EXIT_ON_ERROR")

# Download de worker configuration (based on id)
getworkerconfig ${BUWORKERID} 

# Create the central pgsnapman dump directory
mkdir -p ${ROOTDIR}
mkdir -p ${DUMPSNAP}
mkdir -p ${DEDUPDATA}
mkdir -p ${ROOTDIR}/upload

# Create temp cron job file
echo "# pgsnapman ###################################################################" > ${ROOTDIR}/cron_snapjobs.temp
echo "# pgsnapman jobs created at ${INITIMESTAMP}" >> ${ROOTDIR}/cron_snapjobs.temp
echo "# pgsnapman ###################################################################" >> ${ROOTDIR}/cron_snapjobs.temp

# Add crontab entries for own cron job from worker config
# Must be one line, no filtering
while read line; do
  CACHECONFIGCRON=$(echo "${line}" | cut -d '|' -f 4)
  SINGLEJOBCRON=$(echo "${line}" | cut -d '|' -f 5)
  CLEANCRON=$(echo "${line}" | cut -d '|' -f 6)
  UPLOADCRON=$(echo "${line}" | cut -d '|' -f 7)
done < $ROOTDIR/pgsnap_worker.cron
rm  $ROOTDIR/pgsnap_worker.cron

echo "# pgsnapman worker jobs" >> ${ROOTDIR}/cron_snapjobs.temp
echo "${CACHECONFIGCRON} ${SCRIPTPATH}/pgsnap_cacheconfig" >> ${ROOTDIR}/cron_snapjobs.temp
echo "${CLEANCRON} ${SCRIPTPATH}/pgsnap_clean" >> ${ROOTDIR}/cron_snapjobs.temp
echo "${SINGLEJOBCRON} ${SCRIPTPATH}/pgsnap_singlejob" >> ${ROOTDIR}/cron_snapjobs.temp
echo "${UPLOADCRON} ${SCRIPTPATH}/pgsnap_upload" >> ${ROOTDIR}/cron_snapjobs.temp

# Get postgres instance information for all pgsql instances and cache the data (we need all of them,
# as a one time backup could also be inserted at any time for any host).
${PGSCBIN}/psql ${PGSCHOST} -p ${PGSCPORT} -U ${PGSCUSER} --dbname=${PGSCDB} -F '|' -A -t -c "SELECT id,dns_name,pgport,comment,status, \
  pgsql_superuser,bu_window_start,bu_window_end,pgsql_worker_id_default \
  FROM vw_instance ORDER BY id" > $ROOTDIR/pg_instances.list

# Display information
if [ "$VERBOSITY" == "VERBOSE" ]; then
  echo ""
  echo "+--------------------+"
  echo "| pgsnap_cacheconfig |"
  echo "+--------------------+"
  echo ""
  echo "Config file:              ${CONFIGFILE}"
  echo ""
  echo "PgSnapman worker fqdn:    ${FQDN}"
  echo "PgSnapman worker id:      ${BUWORKERID}"
  echo "PgSnapman config cron     ${CACHECONFIGCRON}"
  echo "PgSnapman clean cron      ${CLEANCRON}"
  echo "PgSnapman single job cron ${SINGLEJOBCRON}"
  echo ""
  echo "Global pgsnapman catalog db"
  echo "  db:   ${PGSCDB}"
  echo "  host: ${PGSCHOST}"
  echo "  port: ${PGSCPORT}"
  echo "  user: ${PGSCUSER}"
  echo ""
  echo "Postgres instances managed by pgsnapman:"
  echo "id|dns_name|pgport|comment|status|pgsuperuser|bu_window_start|bu_window_end|pgsql_worker_id_default"
  cat $ROOTDIR/pg_instances.list
  echo ""
fi

# Create directories for every server listed, write connection info to file, ready to consume
log "INFO" "init" "NEW"
while read line; do
  pgid=$(echo "${line}" | cut -d '|' -f 1)
  pgdns=$(echo "${line}" | cut -d '|' -f 2)
  pgport=$(echo "${line}" | cut -d '|' -f 3)
  comment=$(echo "${line}" | cut -d '|' -f 4)
  pgstatus=$(echo "${line}" | cut -d '|' -f 5)
  pguser=$(echo "${line}" | cut -d '|' -f 6)
  defworkerid=$(echo "${line}" | cut -d '|' -f 9)
  mkdir -p ${DUMPSNAP}/${pgid}_${pgdns}_${pgport}
  pgconninfo=${DUMPSNAP}/${pgid}_${pgdns}_${pgport}/pgconninfo
  if [ "${pgdns}" == "local" ]; then
    echo "PGHOST=" > ${pgconninfo}
  else
    echo "PGHOST="${pgdns} > ${pgconninfo}
  fi
  echo "PGPORT="${pgport} >> ${pgconninfo}
  echo "PGUSER="${pguser} >> ${pgconninfo}
  if [ "${pgstatus}" == "ACTIVE" ]; then
    touch ${DUMPSNAP}/${pgid}_${pgdns}_${pgport}/active
    rm -f ${DUMPSNAP}/${pgid}_${pgdns}_${pgport}/halted
  else
    touch ${DUMPSNAP}/${pgid}_${pgdns}_${pgport}/halted
    rm -f ${DUMPSNAP}/${pgid}_${pgdns}_${pgport}/active
  fi

  # Check the pgsql_instances for unlisted databases, when instance is active and this backup worker is the default backup worker
  if [ "${DATABASEJOBS}" == "AUTO_ADD" ] && [ "${defworkerid}" == "${BUWORKERID}" ] && [ "$(getinstancestatus ${pgid})" == "ACTIVE" ]; then
    createdumpjobs ${pgid} ${pgdns} ${pgport} ${pguser}
  fi

  # Retrieve job list from server
  getjobs ${pgid} ${DUMPSNAP}/${pgid}_${pgdns}_${pgport}

  # Prepare the jobs: create directories, cron entries
  preparejobs ${DUMPSNAP}/${pgid}_${pgdns}_${pgport}
done < $ROOTDIR/pg_instances.list

# Figure out if there are warnings/errors
if [ "`cat ${TOOL_LOG} | grep -E 'WARNING|ERROR'`" == "" ]; then
  JOBWARNINGS=" (without warnings)"
else
  JOBWARNINGS=" (with warnings)"
fi 

# Install new crontab, remove old pgsnap_dump entries, keep other entries
crontab -l > ${ROOTDIR}/crontab.previous
cat ${ROOTDIR}/crontab.previous | grep -v pgsnap | cat - ${ROOTDIR}/cron_snapjobs.temp > ${ROOTDIR}/crontab.latest
crontab ${ROOTDIR}/crontab.latest
if [ "$?" == "0" ]; then
  snaplog "INFO" "processing - crontab properly installed${JOBWARNINGS}"
else
  snaplog "ERROR" "processing - crontab failed to install (format errors)"
fi
rm ${ROOTDIR}/cron_snapjobs.temp

# Create message log for upload, based on logging 
cat  ${TOOL_LOG} | grep -E 'WARNING|ERROR' | awk '{for(i=1;i<=NF;i++) { printf($i); if (i>=1 && i <= 3) {printf("\t")} else { printf(" ")}}; print ""}' > ${ROOTDIR}/pgsnap_cacheconfig.message.dat
ln -s -f ${ROOTDIR}/pgsnap_cacheconfig.message.dat ${ROOTDIR}/upload

snaplog "INFO" "finished"

exit 0

