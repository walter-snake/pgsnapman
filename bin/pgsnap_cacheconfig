#!/bin/bash

# pgsnap_cacheconfig: reads postgres server instances to backup from central configuration database
#                     must run on a backup worker, with passwordless (.pgpass does the job) to the
#                     central configuration database
#
# Commandline options
#   $1 verbosity [SILENT|VERBOSE]
#   $2 action/job type [INIT|CRON|SINGLE]
#   $3 job id for a single job run (required when action = SINGLE)
#
# Actions taken:
# ACTION=INIT
#   New pgsnapman data directory will be built.
#
# ACTION=CRON
#   Downloads and prepares pgsql instance dump directories and scheduled jobs.
#
# ACTION=SINGLE
#   Downloads and prepares pgsql instance dump directories and single run jobs.
#
#   1 connects to central configuration database
#   2 reads which pg instances should be backed up by this host
#   3 creates directory structure and writes connection info for every pg instance
#   4 reads jobs from central configuration database, creates directory structure per database and write cron job file
#
# You may refresh the configuration as often as you like, the actual backup proces runs entirely locally (does not need
# access to the central configuration database). 
# 
# Actual backups will be started by cron on this machine, each backup job is one cron entry, starts script: pgsnap_dump.

# ======================================
# Initialization
# ======================================
VERBOSITY=$1
ACTION=$2
SINGLERUNPID=$3

# Get the script directory (must do this first)
SOURCE="${BASH_SOURCE[0]}"
while [ -h "$SOURCE" ]; do # resolve $SOURCE until the file is no longer a symlink
  DIR="$( cd -P "$( dirname "$SOURCE" )" && pwd )"
  SOURCE="$(readlink "$SOURCE")"
  [[ $SOURCE != /* ]] && SOURCE="$DIR/$SOURCE" # if $SOURCE was a relative symlink, we need to resolve it relative to the path where the symlink file was located
done
SCRIPTPATH="$( cd -P "$( dirname "$SOURCE" )" && pwd )"

# Catalog database needed?
PGSCDB_REQUIRED=YES

# Load functions, will also perform initialization operations
. ${SCRIPTPATH}/pgsnap_lib

# ======================================
# Function definitions (script specific)
# ======================================

# Worker configuration functions
# ------------------------------------------------------------------------

# Init pgsnapman root directory
# Location read from global variable ROOTDIR
function initrootdir {
  echo "PgSNapMan ${ROOTDIR}"

  echo "Initializing data directory..."
  touch "${ROOTDIR}/status"
  if [ "$?" != "0" ]; then
    echo "ERROR PgSnapMan root directory not writable [${ROOTDIR}]"
    exit 1
  fi
  
  echo "  creating work directories"
  # Create the central pgsnapman dump directory
  mkdir -p ${DUMPSNAP}
  mkdir -p ${DEDUPDATA}
  mkdir -p ${ROOTDIR}/upload
  mkdir -p ${ROOTDIR}/script
  mkdir -p ${TEMPDIR}
  mkdir -p ${LOG}
  echo "Done."
}

# Install the new crontab
function installcrontab {
  # Install new crontab, remove old pgsnap_dump entries, keep other entries
  crontab -l > ${TEMPDIR}/crontab.previous
  cat ${TEMPDIR}/crontab.previous | grep -v pgsnap | cat - ${TEMPDIR}/cron_snapjobs.temp > ${TEMPDIR}/crontab.latest
  crontab ${TEMPDIR}/crontab.latest
  if [ "$?" == "0" ]; then
    snaplog "INFO" "processing - crontab properly installed${JOBWARNINGS}"
  else
    snaplog "ERROR" "processing - crontab failed to install (format errors)"
  fi
  rm ${TEMPDIR}/cron_snapjobs.temp
}

# Retrieves the worker configurations from the catalog database
# $1 backup worker id (numeric)
function getworkerconfig {
  # Get worker configuration
  local workercron=`${PGSCBIN}/psql ${PGSCHOST} -p ${PGSCPORT} -U ${PGSCUSER} --dbname=${PGSCDB} -F '|' -A -t -c "SELECT id,dns_name,comment, \
    cron_cacheconfig,cron_singlejob,cron_clean,cron_upload
    FROM vw_worker WHERE id = $1"`
  echo "${workercron}"
}

# Retrieve and set the worker configuration, inclusing creating cron jobs
function updateworkerconfig {

  # Create temp cron job file
  echo "# pgsnapman ###################################################################" > ${TEMPDIR}/cron_snapjobs.temp
  echo "# pgsnapman jobs created at ${INITIMESTAMP}" >> ${TEMPDIR}/cron_snapjobs.temp
  echo "# pgsnapman ###################################################################" >> ${TEMPDIR}/cron_snapjobs.temp

  # Add crontab entries for own cron job from worker config
  row=$(getworkerconfig ${BUWORKERID})
  CACHECONFIGCRON=$(getfieldvalue "${row}" 4)
  SINGLEJOBCRON=$(getfieldvalue "${row}" 5)
  CLEANCRON=$(getfieldvalue "${row}" 6)
  UPLOADCRON=$(getfieldvalue "${row}" 7)

  echo "# pgsnapman worker jobs" >> ${TEMPDIR}/cron_snapjobs.temp
  echo "${CACHECONFIGCRON} ${SCRIPTPATH}/pgsnap_cacheconfig SILENT CRON" >> ${TEMPDIR}/cron_snapjobs.temp
  echo "${CLEANCRON} ${SCRIPTPATH}/pgsnap_clean SILENT" >> ${TEMPDIR}/cron_snapjobs.temp
  echo "${SINGLEJOBCRON} ${SCRIPTPATH}/pgsnap_singlejob SILENT" >> ${TEMPDIR}/cron_snapjobs.temp
  echo "${UPLOADCRON} ${SCRIPTPATH}/pgsnap_upload SILENT" >> ${TEMPDIR}/cron_snapjobs.temp
}

function updatepgsqlinstanceconfig {
  # Get postgres instance information for all pgsql instances and cache the data (we need all of them,
  # as a one time backup could also be inserted at any time for any host).
  ${PGSCBIN}/psql ${PGSCHOST} -p ${PGSCPORT} -U ${PGSCUSER} --dbname=${PGSCDB} -F '|' -A -t -c "SELECT id,dns_name,pgport,comment,status, \
    pgsql_superuser,bu_window_start,bu_window_end,pgsql_worker_id_default \
    FROM vw_instance ORDER BY id" > $ROOTDIR/pg_instances.list

  # Display information
  if [ "$VERBOSITY" == "VERBOSE" ]; then
    echo ""
    echo "+--------------------+"
    echo "| pgsnap_cacheconfig |"
    echo "+--------------------+"
    echo ""
    echo "Config file:              ${CONFIGFILE}"
    echo ""
    echo "PgSnapman worker fqdn:    ${FQDN}"
    echo "PgSnapman worker id:      ${BUWORKERID}"
    echo "PgSnapman config cron     ${CACHECONFIGCRON}"
    echo "PgSnapman clean cron      ${CLEANCRON}"
    echo "PgSnapman single job cron ${SINGLEJOBCRON}"
    echo ""
    echo "Global pgsnapman catalog db"
    echo "  db:   ${PGSCDB}"
    echo "  host: ${PGSCHOST}"
    echo "  port: ${PGSCPORT}"
    echo "  user: ${PGSCUSER}"
    echo ""
    echo "Postgres instances managed by pgsnapman:"
    echo "id|dns_name|pgport|comment|status|pgsuperuser|bu_window_start|bu_window_end|pgsql_worker_id_default"
    cat $ROOTDIR/pg_instances.list
    echo ""
  fi

  # Create directories for every server listed, write connection info to file, ready to consume
  while read line; do
    pgid=$(getfieldvalue "${line}" 1)
    pgdns=$(getfieldvalue "${line}" 2)
    pgport=$(getfieldvalue "${line}" 3)
    comment=$(getfieldvalue "${line}" 4)
    pgstatus=$(getfieldvalue "${line}" 5)
    pguser=$(getfieldvalue "${line}" 6)
    defworkerid=$(getfieldvalue "${line}" 9)
    mkdir -p ${DUMPSNAP}/${pgid}_${pgdns}_${pgport}
    echo ${pgstatus} > ${DUMPSNAP}/${pgid}_${pgdns}_${pgport}/status
  done < $ROOTDIR/pg_instances.list
}

# Job management functions
# ------------------------------------------------------------------------

# Retrieves all databases from a postgres server instance, adds all those that are not listed in the dumpjobs table, with defaults
# - Does not depend on worker, all jobs are checked (a database could purposely be put on another backup worker).
# - SINGLE jobs do count
# $1 = pg instance id
# $2 = pg dns name
# $3 = pg port
# $4 = pg user
function createdumpjobs {
  # Resolve host
  local fqdnhost=${2}
  local pgsqlhost=$(resolvepghost ${2})

  # Find the databases (exclude templates and all database that you cannot connect to)
  # Too lazy too look for the correct postgres binary version, not really important for this plain simple query 
  local DBS=`${PGSCBIN}/psql ${pgsqlhost} -p ${3} -U ${4} --dbname=${MAINTDB} -A -t -c "SELECT datname FROM pg_database WHERE datistemplate = false AND datallowconn = true;"` 2> /dev/null
  if [ "$?" != "0" ]; then
    return
  fi

  # Process the database list
  local c=0
  for line in ${DBS}; do
    local HASJOB=`${PGSCBIN}/psql ${PGSCHOST} -p ${PGSCPORT} -U ${PGSCUSER} --dbname=${PGSCDB} -A -t -c "SELECT get_hasjob(${1}, '${line}');"`
    if [ "${HASJOB}" != "YES" ]; then
      echo "add database: ${fqdnhost}:${3}/${line}"
      local jobid=`${PGSCBIN}/psql ${PGSCHOST} -p ${PGSCPORT} -U ${PGSCUSER} --dbname=${PGSCDB} -A -t -c "SELECT put_dumpjob(${1}, '${line}', 'auto added dump job');"`
      echo "new dump job created with id: ${jobid}"
      let "c = c + 1"
    fi
  done
  if [ ${c} -gt 0 ]; then 
    snaplog "INFO" "processing - auto job: added ${c} dump jobs on worker [${BUWORKERID}] for postgres instance [${fqdnhost}:${3}]"
  fi
}

# Retrieves cron jobs from the configuration database
# $1 postgres host by id
# $2 instance snapshot location (full path)
function getcronjobs {
  # Get postgres worker information for workers managed by this backup worker (based on hostname) and cache the data
  # Make creating/replacing it an atomic operation, a dump could start while writing this file. 
  ${PGSCBIN}/psql ${PGSCHOST} -p ${PGSCPORT} -U ${PGSCUSER} --dbname=${PGSCDB} -F '|' -A -t -c "SELECT id,pgsnap_worker_id,pgsql_instance_id,dbname,dumptype,dumpschema, \
      cron,keep_daily,keep_weekly,keep_monthly,keep_yearly, \
      comment,status,jobtype, \
      pgsnap_worker_dns_name,pgsql_instance_dns_name,pgsql_instance_port,pgsql_instance_superuser,pgsnap_restorejob_id,dumpoptions \
    FROM vw_dumpjob_worker_instance \
    WHERE pgsnap_worker_id = ${BUWORKERID} AND pgsql_instance_id = ${1} \
      AND jobtype = 'CRON' \
    ORDER BY id" > ${2}/dumpjobs.list.temp
  mv ${2}/dumpjobs.list.temp ${2}/dumpjobs.list
  if [ "$VERBOSITY" == "VERBOSE" ]; then
    echo "Job data for postgres instance id: "$1
    echo "-------------------------------------"
    echo "Snapshot base directory: "$2
    echo "id|pgsnap_worker_id|pgsql_instance_id|dbname|dumptype|dumpschema|cron|keep_daily|keep_weekly|keep_monthly|keep_yearly|comment|jobstatus|jobtype|dumpoptions"
    cat ${2}/dumpjobs.list
    echo ""
  fi
}

# Create the cron job entries for the jobs, based on data in the local cache (dumpjobs.list)
# $1 full path to a postgres instance snapshot directory
function createcronjobs {
  if  [ "$VERBOSITY" == "VERBOSE" ]; then
    echo "Creating cron jobs, processing: "$1/dumpjobs.list
  fi
  echo "# pgsnapman pgsnap_dump jobs for "`basename $1` >> ${TEMPDIR}/cron_snapjobs.temp
  while read line; do
    jobid=$(getfieldvalue "${line}" 1)
    pgsqlid=$(getfieldvalue "${line}" 3)
    pgsqldns=$(getfieldvalue "${line}" 16)
    pgsqlport=$(getfieldvalue "${line}" 17)
    pgsqlsuperuser=$(getfieldvalue "${line}" 18)
    dbname=$(getfieldvalue "${line}" 4)
    butype=$(getfieldvalue "${line}" 5)
    schema=$(getfieldvalue "${line}" 6)
    dumpoptions=$(getfieldvalue "${line}" 20)
    cron=$(getfieldvalue "${line}" 7)
    jobstatus=$(getfieldvalue "${line}" 13)
    jobtype=$(getfieldvalue "${line}" 14)
    restorejobid=$(getfieldvalue "${line}" 19)
    # Set schema_part for the dump name
    if [ "${schema}" == "*" ]; then
      schema_part=""
    else
      dumpoptions="--schema=${schema} ${dumpoptions}"
      schema_part="."${schema}
    fi

    # Resolve host
    pgsqlhost=$(resolvepghost ${pgsqldns})
    
    # Display a lot of data in verbose mode
    if  [ "$VERBOSITY" == "VERBOSE" ]; then
      echo "jobid:          ${jobid}"
      echo "pg id:          ${pgsqlid}"
      echo "pg host:        ${pgsqldns}"
      echo "pg port:        ${pgsqlport}"
      echo "pg superuser:   ${pgsqlsuperuser}"
      echo "dbname:         ${dbname}"
      echo "butype:         ${butype}"
      echo "schema:         ${schema}"
      echo "cron:           ${cron}"
      echo "job status:     ${jobstatus}"
      echo "job type:       ${jobtype}"
      echo "dump options:   ${dumpoptions}"
      echo "restore job id: ${restorejobid}"
      echo ""
    fi

    # Create a cron job entry, only if active.
    # If postgres version could not be determined there was an error, but it will emitted as a warning at this stage.
    # The database may be available at the scheduled time, so the dump tool will repeat this test.
    PRE=""
    log "INFO" "job id ${jobid} - ${pgsqlsuperuser}@${pgsqlhost}:${pgsqlport}/${dbname}${schema_part}"
    echo "# INFO pgsnapman job id ${jobid} - ${pgsqlsuperuser}@${pgsqlhost}:${pgsqlport}/${dbname}${schema_part}" >> ${TEMPDIR}/cron_snapjobs.temp 
    # For active jobs, try to verify the database connection and postgresql version
    if [ "${jobstatus}" == "ACTIVE" ]; then
      # Check postgres instance version
      pgversion=$(getpgversion "${pgsqlhost}" ${pgsqlport} ${pgsqlsuperuser} ${dbname})
      if [ "${pgversion}" == "" ]; then
        log "WARNING" "job id ${jobid} - could not connect to postgres instance: ${pgsqlsuperuser}@${pgsqlhost}:${pgsqlport}/${dbname}${schema_part} - pg_dump version can not be verified, leaving job in place"
      else
        # Get pg bin path for this version
        eval pgbinvar=PGBIN${pgversion}
        pgbin=${!pgbinvar}
      fi
      # Verify if pg_dump is available, disable when db connection could be made, version
      # checked and pg_dump not available (this is the only guaranteed problem, databases
      # could become available later).
      if [ "${pgversion}" != "" ]; then
        if [ ! -e ${pgbin}/pg_dump ]; then
          PRE="# "
          log "ERROR" "job id ${jobid} - job is disabled as pg_dump could not be found, check pgbinxx paths in config file ${CONFIGFILE})"
          echo "# ERROR pgsnapman job is disabled as pg_dump could not be found" >> ${TEMPDIR}/cron_snapjobs.temp
        fi
      fi
    fi
    # Emit warning on halted jobs
    if [ "${jobstatus}" == "HALTED" ]; then
      PRE="# "
      log "WARNING" "job id ${jobid} - disabled by user (status HALTED): ${pgsqlsuperuser}@${pgsqlhost}:${pgsqlport}/${dbname}${schema_part}"
      echo "# WARNING pgsnapman job is disabled by user" >> ${TEMPDIR}/cron_snapjobs.temp
    fi
    # Write cron entry
    echo "${PRE}${cron} ${SCRIPTPATH}/pgsnap_dump ${pgsqlid}_${pgsqldns}_${pgsqlport} ${jobid} SILENT CRON" >> ${TEMPDIR}/cron_snapjobs.temp
    log "INFO" "job id ${jobid} - cron entry created: ${PRE}${cron} ${SCRIPTPATH}/pgsnap_dump ${pgsqlid}_${pgsqldns}_${pgsqlport} ${jobid} SILENT CRON"
  done < $1/dumpjobs.list
}

# Entry point for rebuilding the cron job configuration, for every pgsql instance
#   checks for new databases
#   retrieve scheduled job information
#   creates the cron entries
function updatecronjobconfig {
  log "INFO" "init" "NEW"
  while read line; do
    # Need to collect the pgsql instance data
    pgid=$(getfieldvalue "${line}" 1)
    pgdns=$(getfieldvalue "${line}" 2)
    pgport=$(getfieldvalue "${line}" 3)
    comment=$(getfieldvalue "${line}" 4)
    pgstatus=$(getfieldvalue "${line}" 5)
    pguser=$(getfieldvalue "${line}" 6)
    defworkerid=$(getfieldvalue "${line}" 9)

    # Check the pgsql_instances for unlisted databases, when instance is active and this backup worker is the default backup worker
    if [ "${AUTO_DUMPJOB}" == "YES" ] && [ "${defworkerid}" == "${BUWORKERID}" ] && [ "$(getinstancestatus ${pgid}_${pgdns}_${pgport})" == "ACTIVE" ]; then
      createdumpjobs ${pgid} ${pgdns} ${pgport} ${pguser}
    fi

    # Retrieve job list from server
    getcronjobs ${pgid} ${DUMPSNAP}/${pgid}_${pgdns}_${pgport}

    # Prepare the jobs: create directories, cron entries
    createcronjobs ${DUMPSNAP}/${pgid}_${pgdns}_${pgport}
  done < $ROOTDIR/pg_instances.list
}

# Set up the job list for the single jobs that have to be executed (both dump and restore)
function updatesinglejobconfig {
  local sql="SELECT id,pgsnap_worker_id,pgsql_instance_id,dbname,dumptype,dumpschema \
    ,cron,keep_daily,keep_weekly,keep_monthly,keep_yearly \
    ,comment,status \
    ,jobtype,pgsnap_worker_dns_name,pgsql_instance_dns_name,pgsql_instance_port \
    ,pgsql_instance_superuser \
    ,pgsnap_restorejob_id,dumpoptions \
    FROM vw_dumpjob_worker_instance \
    WHERE id NOT IN (select jobid from pgsnap_singlerun where jobclass = 'DUMP') \
    AND pgsnap_worker_id = ${BUWORKERID} \
    AND jobtype = 'SINGLE' \
    ORDER BY id;"
  ${PGSCBIN}/psql ${PGSCHOST} -p ${PGSCPORT} -U ${PGSCUSER} --dbname=${PGSCDB} -F '|' -A -t -c "${sql}" > ${TEMPDIR}/${SINGLERUNPID}.singledumpjobs.list
  c=0
  while read line; do
    let "c = c + 1"
  done < ${TEMPDIR}/${SINGLERUNPID}.singledumpjobs.list
  if [ $c -eq 0 ]; then
    local result=NO
  else
    local result=YES
  fi
  echo ${result}
}

# ================================================================================
# MAIN
# ===============================================================================

# Tool log
TOOL_LOG=${LOGDIR}/$$.pgsnap_cacheconfig.log 

# Check action
if [  "${ACTION}" == "" ]; then
  echo "ERROR no action provided"
  exit 1
elif [ "${ACTION}" == "INIT" ]; then
  initrootdir

  # Get initial worker config (which will start up the regular cron jobs)
  echo "Reading worker settings from database for id [${BUWORKERID}]"
  echo "Update the worker configuration..."
  updateworkerconfig
  echo "Installed scheduled jobs."

  # Install the new crontab, so that worker cron jobs will fire up
  installcrontab

  # And done
  exit 0
fi

# Before running any other action, always update the worker and psql instance configuration, as
# the directories must be in place when starting a dump job.

# Depending on ACTION:
#  CRON will check for databases without jobs, download jobtype CRON jobs, prepare dirs and schedule them
#  SINGLE will download jobtype SINGLE jobs to a specified file, prepare dirs without writing cron entries
if [ "${ACTION}" == "CRON" ]; then
  updateworkerconfig
  updatepgsqlinstanceconfig
  updatecronjobconfig
  installcrontab
elif [ "${ACTION}" == "SINGLE" ] &&  [ "${SINGLERUNPID}" == "" ]; then
  # Error condition
  snaplog "ERROR" "can not run in SINGLE mode without SINGLERUNPID"
  exit 1
elif [ "${ACTION}" == "SINGLE" ]; then
  # First retrieve single jobs
  jobspending=$(updatesinglejobconfig)
  # If anything to do
  if [ "${jobspending}" == "YES" ]; then
    updatepgsqlinstanceconfig
  fi
fi

# Round up with a bit of logging
# Figure out if there are warnings/errors
if [ "`cat ${TOOL_LOG} | grep -E 'WARNING|ERROR'`" == "" ]; then
  JOBWARNINGS=" (without warnings)"
else
  JOBWARNINGS=" (with warnings)"
fi 

# Transform message log for upload, based on logging (with an awk one liner)
# Create the upload data file in upload, we keep the log so we don't have to keep the upload dat file (write it straight to upload dir, then it will be removed)
preparecatalogdata "`cat  ${TOOL_LOG} | grep -E 'WARNING|ERROR' | awk '{for(i=1;i<=NF;i++) { printf($i); if (i>=1 && i <= 3) {printf("\t")} else { printf(" ")}}; print ""}'`" ${ROOTDIR}/upload/$$.pgsnap_cacheconfig.message.dat NEW

# We also append the session log to the master log, and delete session log
cat ${TOOL_LOG} >> ${LOGDIR}/pgsnap_cacheconfig.log
rm -f ${TOOL_LOG}

snaplog "INFO" "finished"

exit 0

