#!/bin/bash

# pgsnap_cacheconfig: reads postgres server instances to backup from central configuration database
#                     must run on a backup worker, with passwordless (.pgpass does the job) to the
#                     central configuration database
#
# Commandline options
#   $1 verbosity [SILENT|VERBOSE]
#   $2 action/startup [INIT|CRON|SINGLE]
#   $3 job id for a single job run (required when action = SINGLE)
#
# Actions taken:
# ACTION=INIT
#   New pgsnapman data directory will be built.
#
# ACTION=CRON
#   Downloads and prepares pgsql instance dump directories and scheduled jobs (dump and restore).
#
# ACTION=SINGLE
#   Downloads and prepares pgsql instance dump directories and single run jobs (dump only).
#
#   1 connects to central configuration database
#   2 reads which pg instances should be backed up by this host
#   3 creates directory structure and writes connection info for every pg instance
#   4 reads jobs from central configuration database, creates directory structure per database and write cron job file
#
# You may refresh the configuration as often as you like, the actual backup proces runs entirely locally (does not need
# access to the central configuration database). 
# 
# Actual backups will be started by cron on this machine, each backup job is one cron entry, starts script: pgsnap_dump.

# ======================================
# Initialization
# ======================================
VERBOSITY=$1
ACTION=$2
SINGLERUNPID=$3

# Get the script directory (must do this first)
SOURCE="${BASH_SOURCE[0]}"
while [ -h "$SOURCE" ]; do # resolve $SOURCE until the file is no longer a symlink
  DIR="$( cd -P "$( dirname "$SOURCE" )" && pwd )"
  SOURCE="$(readlink "$SOURCE")"
  [[ $SOURCE != /* ]] && SOURCE="$DIR/$SOURCE" # if $SOURCE was a relative symlink, we need to resolve it relative to the path where the symlink file was located
done
SCRIPTPATH="$( cd -P "$( dirname "$SOURCE" )" && pwd )"

# Skip logging without configured directory
if [ "${ACTION}" == "INIT" ]; then LOGINIT="NO"; fi

# Catalog database needed?
PGSCDB_REQUIRED=YES

# Load functions, will also perform initialization operations
. ${SCRIPTPATH}/pgsnap_lib

# ======================================
# Function definitions (script specific)
# ======================================

# Worker configuration functions
# ------------------------------------------------------------------------

# Init pgsnapman root directory
# Location read from global variable ROOTDIR
function initrootdir {
  echo "PgSNapMan ${ROOTDIR}"

  echo "Initializing data directory..."
  touch "${ROOTDIR}/status"
  if [ "$?" != "0" ]; then
    echo "ERROR PgSnapMan root directory not writable [${ROOTDIR}]"
    cleanupexit 1
  fi
  
  echo "  creating work directories"
  # Create the central pgsnapman dump directory
  mkdir -p ${DUMPSNAP}
  mkdir -p ${DEDUPDATA}
  mkdir -p ${UPLOADDIR}
  mkdir -p ${SCRIPTDIR}
  mkdir -p ${TEMPDIR}
  mkdir -p ${LOGDIR}
  echo "Done."

  echo -n "${BUWORKERID}" > ${ROOTDIR}/workerid
}

# Install the new crontab
function installcrontab {
  # Install new crontab, remove old pgsnap_dump entries, keep other entries
  crontab -l > ${TEMPDIR}/crontab.previous
  cat ${TEMPDIR}/crontab.previous | grep -v pgsnap | cat - ${TEMPDIR}/$$.0.cron_snapjobs.temp > ${TEMPDIR}/crontab.latest
  crontab ${TEMPDIR}/crontab.latest
  if [ "$?" == "0" ]; then
    snaplog "INFO" "processing - crontab properly installed${JOBWARNINGS}"
  else
    snaplog "ERROR" "processing - crontab failed to install (format errors)"
  fi
  rm ${TEMPDIR}/$$.0.cron_snapjobs.temp
}

# Retrieves the worker configurations from the catalog database
# $1 backup worker id (numeric)
function getworkerconfig {
  # Get worker configuration
  local workercron=`${PGSCBIN}/psql ${PGSCHOST} -p ${PGSCPORT} -U ${PGSCUSER} --dbname=${PGSCDB} -F '|' -A -t -c "SELECT id,dns_name,comment, \
    cron_cacheconfig,cron_singlejob,cron_clean,cron_upload
    FROM vw_worker WHERE id = $1"`
  echo "${workercron}"
}

# Retrieve and set the worker configuration, inclusing creating cron jobs
function updateworkerconfig {

  # Create temp cron job file
  echo "# pgsnapman ###################################################################" > ${TEMPDIR}/$$.0.cron_snapjobs.temp
  echo "# pgsnapman jobs created at ${INITIMESTAMP}" >> ${TEMPDIR}/$$.0.cron_snapjobs.temp
  echo "# pgsnapman ###################################################################" >> ${TEMPDIR}/$$.0.cron_snapjobs.temp

  # Add crontab entries for own cron job from worker config
  row=$(getworkerconfig ${BUWORKERID})
  CACHECONFIGCRON=$(getfieldvalue "${row}" 4)
  SINGLEJOBCRON=$(getfieldvalue "${row}" 5)
  CLEANCRON=$(getfieldvalue "${row}" 6)
  UPLOADCRON=$(getfieldvalue "${row}" 7)

  echo "# pgsnapman worker jobs" >> ${TEMPDIR}/$$.0.cron_snapjobs.temp
  echo "${CACHECONFIGCRON} ${SCRIPTPATH}/pgsnap_cacheconfig SILENT CRON" >> ${TEMPDIR}/$$.0.cron_snapjobs.temp
  echo "${CLEANCRON} ${SCRIPTPATH}/pgsnap_clean SILENT" >> ${TEMPDIR}/$$.0.cron_snapjobs.temp
  echo "${SINGLEJOBCRON} ${SCRIPTPATH}/pgsnap_singlejob SILENT" >> ${TEMPDIR}/$$.0.cron_snapjobs.temp
  echo "${UPLOADCRON} ${SCRIPTPATH}/pgsnap_upload SILENT" >> ${TEMPDIR}/$$.0.cron_snapjobs.temp
}

function updatepgsqlinstanceconfig {
  local line
  # Get postgres instance information for all pgsql instances and cache the data (we need all of them,
  # as a one time backup could also be inserted at any time for any host).
  ${PGSCBIN}/psql ${PGSCHOST} -p ${PGSCPORT} -U ${PGSCUSER} --dbname=${PGSCDB} -F '|' -A -t -c "SELECT id,dns_name,pgport,comment,status, \
    pgsql_superuser,bu_window_start,bu_window_end,pgsql_worker_id_default \
    FROM vw_instance ORDER BY id" > $ROOTDIR/pg_instances.list

  # Display information
  if [ "$VERBOSITY" == "VERBOSE" ]; then
    echo ""
    echo "+--------------------+"
    echo "| pgsnap_cacheconfig |"
    echo "+--------------------+"
    echo ""
    echo "Config file:              ${CONFIGFILE}"
    echo ""
    echo "PgSnapman worker fqdn:    ${FQDN}"
    echo "PgSnapman worker id:      ${BUWORKERID}"
    echo "PgSnapman config cron     ${CACHECONFIGCRON}"
    echo "PgSnapman clean cron      ${CLEANCRON}"
    echo "PgSnapman single job cron ${SINGLEJOBCRON}"
    echo ""
    echo "Global pgsnapman catalog db"
    echo "  db:   ${PGSCDB}"
    echo "  host: ${PGSCHOST}"
    echo "  port: ${PGSCPORT}"
    echo "  user: ${PGSCUSER}"
    echo ""
    echo "Postgres instances managed by pgsnapman:"
    echo "id|dns_name|pgport|comment|status|pgsuperuser|bu_window_start|bu_window_end|pgsql_worker_id_default"
    cat $ROOTDIR/pg_instances.list
    echo ""
  fi

  # Create directories for every server listed, write connection info to file, ready to consume
  while read line; do
    pgid=$(getfieldvalue "${line}" 1)
    pgdns=$(getfieldvalue "${line}" 2)
    pgport=$(getfieldvalue "${line}" 3)
    comment=$(getfieldvalue "${line}" 4)
    pgstatus=$(getfieldvalue "${line}" 5)
    pguser=$(getfieldvalue "${line}" 6)
    defworkerid=$(getfieldvalue "${line}" 9)
    mkdir -p ${DUMPSNAP}/${pgid}_${pgdns}_${pgport}
    echo ${pgstatus} > ${DUMPSNAP}/${pgid}_${pgdns}_${pgport}/status
  done < $ROOTDIR/pg_instances.list
}

# Job management functions
# ------------------------------------------------------------------------

# Retrieves all databases from a postgres server instance, adds all those that are not listed in the dumpjobs table, with defaults
# - Does not depend on worker, all jobs are checked (a database could purposely be put on another backup worker).
# - SINGLE jobs do count
# $1 = pg instance id
# $2 = pg dns name
# $3 = pg port
# $4 = pg user
function createdumpjobs {
  local line
  # Resolve host
  local fqdnhost=${2}
  local pgsqlhost=$(resolvepghost ${2})

  # Find the databases (exclude templates and all database that you cannot connect to)
  # Too lazy too look for the correct postgres binary version, not really important for this plain simple query 
  local DBS=`${PGSCBIN}/psql ${pgsqlhost} -p ${3} -U ${4} --dbname=${MAINTDB} -A -t -c "SELECT datname FROM pg_database WHERE datistemplate = false AND datallowconn = true;"` 2> /dev/null
  if [ "$?" != "0" ]; then
    return
  fi

  # Process the database list
  local c=0
  for line in ${DBS}; do
    local HASJOB=`${PGSCBIN}/psql ${PGSCHOST} -p ${PGSCPORT} -U ${PGSCUSER} --dbname=${PGSCDB} -A -t -c "SELECT get_hasjob(${1}, '${line}');"`
    if [ "${HASJOB}" != "YES" ]; then
      echo "add database: ${fqdnhost}:${3}/${line}"
      local jobid=`${PGSCBIN}/psql ${PGSCHOST} -p ${PGSCPORT} -U ${PGSCUSER} --dbname=${PGSCDB} -A -t -c "SELECT put_dumpjob(${1}, '${line}', 'auto added dump job', '${AUTO_DUMPJOB_STATUS}');"`
      echo "new dump job created with id: ${jobid}"
      let "c = c + 1"
    fi
  done
  if [ ${c} -gt 0 ]; then 
    snaplog "INFO" "processing - auto job: added ${c} dump jobs on worker [${BUWORKERID}] for postgres instance [${fqdnhost}:${3}]"
  fi
}

# Retrieves cron jobs from the configuration database
# $1 postgres host by id
# $2 instance snapshot location (full path)
function getcronjobs {
  # Get postgres worker information for workers managed by this backup worker (based on hostname) and cache the data
  # Make creating/replacing it an atomic operation, a dump could start while writing this file. 
  ${PGSCBIN}/psql ${PGSCHOST} -p ${PGSCPORT} -U ${PGSCUSER} --dbname=${PGSCDB} -F '|' -A -t -c "SELECT id,pgsnap_worker_id,pgsql_instance_id,dbname,dumptype,dumpschema, \
      cron,keep_daily,keep_weekly,keep_monthly,keep_yearly, \
      comment,status,jobtype, \
      pgsnap_worker_dns_name,pgsql_instance_dns_name,pgsql_instance_port,pgsql_instance_superuser,pgsnap_restorejob_id,dumpoptions \
    FROM vw_dumpjob_worker_instance \
    WHERE pgsnap_worker_id = ${BUWORKERID} \
      AND cron NOT LIKE '* * * * *'  \
      AND pgsql_instance_id = ${1} \
    ORDER BY id" > ${2}/dumpjobs.list.temp
  mv ${2}/dumpjobs.list.temp ${2}/dumpjobs.list


  # Be talkative
  if [ "$VERBOSITY" == "VERBOSE" ]; then
    echo "Job data for postgres instance id: "$1
    echo "-------------------------------------"
    echo "Snapshot base directory: "$2
    echo "Dump jobs"
    echo "id|pgsnap_worker_id|pgsql_instance_id|dbname|dumptype|dumpschema|cron|keep_daily|keep_weekly|keep_monthly|keep_yearly|comment|jobstatus|jobtype|dumpoptions"
    cat ${2}/dumpjobs.list
    echo ""
  fi

}

# Create the cron job entries for the jobs, based on data in the local cache (dumpjobs.list)
# $1 full path to a postgres instance snapshot directory
function createcronjobs {
  local line
  if  [ "$VERBOSITY" == "VERBOSE" ]; then
    echo "Creating cron jobs, processing: "$1/dumpjobs.list
  fi
  echo "# pgsnapman pgsnap_dump jobs for "`basename $1` >> ${TEMPDIR}/$$.0.cron_snapjobs.temp
  #cat $1/dumpjobs.list | grep -v "\* \* \* \* \*" | while read line; do
  while read line; do
    jobid=$(getfieldvalue "${line}" 1)
    pgsqlid=$(getfieldvalue "${line}" 3)
    pgsqldns=$(getfieldvalue "${line}" 16)
    pgsqlport=$(getfieldvalue "${line}" 17)
    pgsqlsuperuser=$(getfieldvalue "${line}" 18)
    dbname=$(getfieldvalue "${line}" 4)
    butype=$(getfieldvalue "${line}" 5)
    schema=$(getfieldvalue "${line}" 6)
    dumpoptions=$(getfieldvalue "${line}" 20)
    cron=$(getfieldvalue "${line}" 7)
    jobstatus=$(getfieldvalue "${line}" 13)
    jobtype=$(getfieldvalue "${line}" 14)
    restorejobid=$(getfieldvalue "${line}" 19)
    # Set schema_part for the dump name
    if [ "${schema}" == "*" ] || [ "${butype}" == "SCRIPT" ]; then
      schema_part=""
    else
      dumpoptions="--schema=${schema} ${dumpoptions}"
      schema_part="."${schema}
    fi

    # Resolve host
    pgsqlhost=$(resolvepghost ${pgsqldns})
    
    # Display a lot of data in verbose mode
    if  [ "$VERBOSITY" == "VERBOSE" ]; then
      echo "jobid:          ${jobid}"
      echo "pg id:          ${pgsqlid}"
      echo "pg host:        ${pgsqldns}"
      echo "pg port:        ${pgsqlport}"
      echo "pg superuser:   ${pgsqlsuperuser}"
      echo "dbname:         ${dbname}"
      echo "butype:         ${butype}"
      echo "schema:         ${schema}"
      echo "cron:           ${cron}"
      echo "job status:     ${jobstatus}"
      echo "job type:       ${jobtype}"
      echo "dump options:   ${dumpoptions}"
      echo "restore job id: ${restorejobid}"
      echo ""
    fi

    # Create a cron job entry, only if active.
    # If postgres version could not be determined there was an error, but it will emitted as a warning at this stage.
    # The database may be available at the scheduled time, so the dump tool will repeat this test.
    PRE=""
    # For active jobs, try to verify the database connection and postgresql version
    if [ "${jobstatus}" == "HALTED" ]; then
      log "WARNING" "job id ${jobid} - disabled by user (status HALTED): ${pgsqlsuperuser}@${pgsqldns}:${pgsqlport}/${dbname}${schema_part}"
    fi
    if [ "${jobstatus}" == "ACTIVE" ]; then
      echo "# INFO pgsnapman job id ${jobid} - ${pgsqlsuperuser}@${pgsqldns}:${pgsqlport}/${dbname}${schema_part}" >> ${TEMPDIR}/$$.0.cron_snapjobs.temp 
      log "INFO" "job id ${jobid} - ${pgsqlsuperuser}@${pgsqldns}:${pgsqlport}/${dbname}${schema_part}"
      # Check postgres instance version
      pgversion=$(getpgversion "${pgsqlhost}" ${pgsqlport} ${pgsqlsuperuser} ${dbname})
      if [ "${pgversion}" == "" ]; then
        log "WARNING" "job id ${jobid} - could not connect to postgres instance: ${pgsqlsuperuser}@${pgsqldns}:${pgsqlport}/${dbname}${schema_part} - pg_dump version can not be verified, leaving job in place"
      else
        # Get pg bin path for this version
        eval pgbinvar=PGBIN${pgversion}
        pgbin=${!pgbinvar}
      fi
      # Verify if pg_dump is available, disable when db connection could be made, version
      # checked and pg_dump not available (this is the only guaranteed problem, databases
      # could become available later).
      if [ "${pgversion}" != "" ]; then
        if [ ! -e ${pgbin}/pg_dump ]; then
          PRE="# "
          log "ERROR" "job id ${jobid} - job is disabled as pg_dump could not be found, check PGBINxx paths in config file ${CONFIGFILE})"
          echo "# ERROR pgsnapman job is disabled as pg_dump could not be found" >> ${TEMPDIR}/$$.0.cron_snapjobs.temp
        fi
      fi
    # Probably better to leave them out, only good for debugging: emit warning on halted jobs
#    else [ "${jobstatus}" == "HALTED" ]; then
#      PRE="# "
#      log "WARNING" "job id ${jobid} - disabled by user (status HALTED): ${pgsqlsuperuser}@${pgsqlhost}:${pgsqlport}/${dbname}${schema_part}"
#      echo "# WARNING pgsnapman job is disabled by user" >> ${TEMPDIR}/$$.0.cron_snapjobs.temp
    # Write cron entry
      echo "${PRE}${cron} ${SCRIPTPATH}/pgsnap_dump ${pgsqlid}_${pgsqldns}_${pgsqlport} ${jobid} SILENT CRON" >> ${TEMPDIR}/$$.0.cron_snapjobs.temp
      log "INFO" "job id ${jobid} - cron entry created: ${PRE}${cron} ${SCRIPTPATH}/pgsnap_dump ${pgsqlid}_${pgsqldns}_${pgsqlport} ${jobid} SILENT CRON"
    fi
  done < $1/dumpjobs.list
}

# Entry point for rebuilding the cron job configuration, for every pgsql instance
#   checks for new databases
#   retrieve scheduled job information
#   creates the cron entries
function updatecronjobconfig {
  local line
  log "INFO" "init" "NEW"
  while read line; do
    # Need to collect the pgsql instance data
    pgid=$(getfieldvalue "${line}" 1)
    pgdns=$(getfieldvalue "${line}" 2)
    pgport=$(getfieldvalue "${line}" 3)
    comment=$(getfieldvalue "${line}" 4)
    pgstatus=$(getfieldvalue "${line}" 5)
    pguser=$(getfieldvalue "${line}" 6)
    defworkerid=$(getfieldvalue "${line}" 9)

    # Check the pgsql_instances for unlisted databases, when instance is active and this backup worker is the default backup worker
    if [ "${AUTO_DUMPJOB}" == "YES" ] && [ "${defworkerid}" == "${BUWORKERID}" ] && [ "$(getinstancestatus ${pgid}_${pgdns}_${pgport})" == "ACTIVE" ]; then
      createdumpjobs ${pgid} ${pgdns} ${pgport} ${pguser}
    fi

    # Retrieve job list from server
    getcronjobs ${pgid} ${DUMPSNAP}/${pgid}_${pgdns}_${pgport}

    # Prepare the jobs: create directories, cron entries
    createcronjobs ${DUMPSNAP}/${pgid}_${pgdns}_${pgport}
  done < $ROOTDIR/pg_instances.list
}

# Set up the job list for the single jobs that have to be executed (both dump and restore)
function updatesinglejobconfig {
  local line
  # cron = '* * * * *' is the clue to run a job on the minutely scheduler, only valid for SINGLE (RUN) JOBS
  local sql="SELECT id,pgsnap_worker_id,pgsql_instance_id,dbname,dumptype,dumpschema \
    ,cron,keep_daily,keep_weekly,keep_monthly,keep_yearly \
    ,comment,status \
    ,jobtype,pgsnap_worker_dns_name,pgsql_instance_dns_name,pgsql_instance_port \
    ,pgsql_instance_superuser \
    ,pgsnap_restorejob_id,dumpoptions \
    FROM vw_dumpjob_worker_instance \
    WHERE cron = '* * * * *' \
      AND jobtype = 'SINGLE' \
      AND status = 'ACTIVE' \
      AND pgsnap_worker_id = ${BUWORKERID} \
    ORDER BY id;"
  ${PGSCBIN}/psql ${PGSCHOST} -p ${PGSCPORT} -U ${PGSCUSER} --dbname=${PGSCDB} -F '|' -A -t -c "${sql}" > ${TEMPDIR}/${SINGLERUNPID}.0.singledumpjobs.list
  c=0
  while read line; do
    let "c = c + 1"
  done < ${TEMPDIR}/${SINGLERUNPID}.0.singledumpjobs.list
  if [ $c -eq 0 ]; then
    local result=NO
  else
    local result=YES
  fi
  echo ${result}
}

# Get restore cron jobs for this backup worker 
# restore jobs from dump catalog entries that are created by this worker (we don't have other data)
# include all CRON jobs, restore jobs always contact the catalog database for their configuration
function addrestorejobs {
  # Restore jobs (temp, will not be cached)
  talk "Checking for cron run dump jobs"
  sql="select r.id as restore_id, c.id as catalog_id, r.cron, c.bu_name, c.starttime, r.dest_dbname, r.restoreschema \
    , r.restoretype, r.restoreoptions from pgsnap_restorejob r join pgsnap_catalog c on r.pgsnap_catalog_id = c.id \
    where r.cron NOT LIKE '* * * * *' and c.bu_worker_id = ${BUWORKERID};"
  ${PGSCBIN}/psql ${PGSCHOST} -p ${PGSCPORT} -U ${PGSCUSER} --dbname=${PGSCDB} -F '|' -A -t -c "${sql}" --output="${TEMPDIR}/$$.0.restorejobs.list"
  # Be talkative
  if [ "$VERBOSITY" == "VERBOSE" ]; then
    echo "Restore jobs"
    echo "id|catalog_id|cron|bu_name|starttime|dest_dbname|restoreschema|restoretype|restoreoptions"
    cat ${TEMPDIR}/$$.0.restorejobs.list
    echo ""
  fi

  echo "# pgsnapman restore jobs" >> ${TEMPDIR}/$$.0.cron_snapjobs.temp
  local line
  while read line; do
    echo "$(getfieldvalue "${line}" 3) ${SCRIPTPATH}/pgsnap_restore $(getfieldvalue "${line}" 1) $(getfieldvalue "${line}" 2) SILENT CRON" >> ${TEMPDIR}/$$.0.cron_snapjobs.temp
  done < ${TEMPDIR}/$$.0.restorejobs.list
}

# ================================================================================
# MAIN
# ===============================================================================
# Tool log
TOOL_LOG=${LOGDIR}/$$.pgsnap_cacheconfig.log 

# Check action
if [  "${ACTION}" == "" ]; then
  echo "ERROR no action provided"
  cleanupexit 1
elif [ "${ACTION}" == "INIT" ]; then
  # Check grep
  . ${SCRIPTPATH}/pgsnap_testgrep
  if [ "${TESTGREP}" == "FAILED" ]; then
    cleanupexit 1
  fi

  # Set up data/root dir
  initrootdir

  # Get initial worker config (which will start up the regular cron jobs)
  echo "Searching worker settings for hostname        [${FQDN}]"
  echo "Reading worker settings from database for id  [${BUWORKERID}]"
  echo "Update the worker configuration..."
  updateworkerconfig
  echo "Installed scheduled jobs."

  # Install the new crontab, so that worker cron jobs will fire up
  installcrontab

  # Write log entry to database
  MSG="Initialized work directory: ${ROOTDIR}. Hint: manually run pgsnap_cacheconfig VERBOSE CRON at the worker to verify job creation, but PgSnapMan should start working now."
  preparecatalogdata "`date '+%Y%m%dT%H%M%S%z'`\tINFO\t${TOOLNAME}\t${MSG}\tCACHE_CONFIG\t-1\t${BUWORKERID}" "${UPLOADDIR}/$$.pgsnap_cache_config.message.dat" "NEW" ""
  ${SCRIPTPATH}/pgsnap_upload

  # And done
  cleanupexit 0
else
  if [ ! -e "${ROOTDIR}/workerid" ]; then
    echo "ERROR pgsnapman not initialized (hint: run 'pgsnap_cacheconfig VERBOSE INIT')"
    cleanupexit 1
  fi
fi

# Check if already running, quit if so, exit code 0 (its not an error).
JOBID=0
if [ "$(isjobrunning ${JOBID})" == "0" ]; then
  lock ${JOBID} "${0} ${1} ${2} ${3} ${4} ${5}"
else
  cleanupexit 0
fi

# Before running any other action, always update the worker and psql instance configuration, as
# the directories must be in place when starting a dump job.

# Depending on ACTION:
#  CRON will check for databases without jobs, download jobtype CRON jobs, prepare dirs and schedule them
#  SINGLE will download jobtype SINGLE jobs to a specified file, prepare dirs without writing cron entries
if [ "${ACTION}" == "CRON" ]; then
  updateworkerconfig
  updatepgsqlinstanceconfig
  updatecronjobconfig
  addrestorejobs
  installcrontab
elif [ "${ACTION}" == "SINGLE" ] &&  [ "${SINGLERUNPID}" == "" ]; then
  # Error condition
  talk "ERROR can not run in SINGLE mode without SINGLERUNPID"
  snaplog "ERROR" "can not run in SINGLE mode without SINGLERUNPID"
  cleanupexit 1
elif [ "${ACTION}" == "SINGLE" ]; then
  # First retrieve single jobs
  jobspending=$(updatesinglejobconfig)
  # If anything to do
  if [ "${jobspending}" == "YES" ]; then
    updatepgsqlinstanceconfig
  fi
fi

# If there's log info written, do something sensible
# Round up with a bit of logging
if [ -e ${TOOL_LOG} ]; then
  # Figure out if there are warnings/errors
  if [ "`cat ${TOOL_LOG} | ${GRPPROG} ${GRPFLG} 'WARNING|ERROR'`" == "" ]; then
    JOBWARNINGS=" (without warnings)"
  else
    # With warnings: upload to database (messages)
    JOBWARNINGS=" (with warnings)"
    # Transform message log for upload, based on logging (with an awk one liner)
    # Create the upload data file in upload, we keep the log so we don't have to keep the upload dat file (write it straight to upload dir, then it will be removed)
    preparecatalogdata "`cat ${TOOL_LOG} | ${GRPPROG} ${GRPFLG} 'WARNING|ERROR' | awk WID=${BUWORKERID} '{for(i=1;i<=NF;i++) { printf($i); if (i>=1 && i <= 3) {printf("\t")} else { printf(" ")}}; print "\tCACHE_CONFIG\t-1\t"WID; }'`" ${UPLOADDIR}/$$.pgsnap_cacheconfig.message.dat NEW
  fi 

  # We also append the session log to the master log, and delete session log
  cat ${TOOL_LOG} >> ${LOGDIR}/pgsnap_cacheconfig.log
  rm -f ${TOOL_LOG}
fi

snaplog "INFO" "finished"

cleanupexit 0

